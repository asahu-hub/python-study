{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "# Important note!\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "YOUR_ID = \"\" # Please enter your GT login, e.g., \"rvuduc3\" or \"gtg911x\"\n",
    "COLLABORATORS = [] # list of strings of your collaborators' IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b11295002cc2b9549d6a2b01721b6701",
     "grade": true,
     "grade_id": "who__test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_CHECK_ID = re.compile (r'''[a-zA-Z]+\\d+|[gG][tT][gG]\\d+[a-zA-Z]''')\n",
    "assert RE_CHECK_ID.match (YOUR_ID) is not None\n",
    "\n",
    "collab_check = [RE_CHECK_ID.match (i) is not None for i in COLLABORATORS]\n",
    "assert all (collab_check)\n",
    "\n",
    "del collab_check\n",
    "del RE_CHECK_ID\n",
    "del re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter / IPython version check.** The following code cell verifies that you are using the correct version of Jupyter/IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression via the PCA and the SVD\n",
    "\n",
    "The main topic of this class is a data analysis method referred to as _Principal Components Analysis_ (PCA). The method requires computing the eigenvectors of a certain matrix; one way to compute those eigenvectors is to use a special factorization from linear algebra called the _Singular Value Decomposition_ (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Data \"compression\"\n",
    "\n",
    "In previous classes, we've looked at 3 major tasks in data analysis: _regression_, _classification_, and _clustering_. Beyond these, the last problem area you'll consider in our class is what we'll call _compression_.\n",
    "\n",
    "At a high level, the term compression simply refers to finding any compact representation of the data. Such representations can help us in two ways. First, it can make the data set smaller and therefore faster to process or analyze. Secondly, choosing a clever representation can reveal hidden structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a concrete example, consider the problem of _dimensionality reduction_: given a $d$-dimensional data set, we wish to transform it into a smaller $k$-dimensional data set where $k \\leq d$.\n",
    "\n",
    "Choosing the $k$ axes in a clever way might even reveal  structure that is hard to see in all $d$ original dimensions. For instance, look at the examples at the \"visualizing PCA\" website:\n",
    "\n",
    "http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "Here is one of those examples, which is nutritional data gathered in a study of four countries of the United Kingdom. (Researchers tabulated the average number of grams consumed per week by an individual living in a particular country, broken down along various food and drink categories.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_uk = pd.read_csv ('uk-nutrition-data.csv')\n",
    "print (\"{} x {} table of data:\".format (df_uk.shape[0], df_uk.shape[1]))\n",
    "display (df_uk.head ())\n",
    "print (\"...\")\n",
    "\n",
    "fig, axes = plt.subplots (1, 4, figsize=(12, 6), sharey=True)\n",
    "countries = df_uk.columns.difference (['Product'])\n",
    "for i in range (len (countries)):\n",
    "    sns.barplot (x=countries[i], y='Product', data=df_uk, ax=axes[i])\n",
    "    axes[i].set_ylabel (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the countries differ in any significant way? Looking only at the bar charts, it is probably hard to tell unless you are very perceptive, and in any case, visual inspection is a very _ad hoc_ technique. Is there a more systematic way?\n",
    "\n",
    "The method of _principal components analysis_ (PCA) is one such technique. For this example, it would start by viewing these data as four (4) data points, one for each country, embedded in a 17-dimensional space (one dimension per food category). The following page will help build your intuition for PCA; the notes that then follow below show formally how PCA works and derives an algorithm to compute it.\n",
    "\n",
    "http://setosa.io/ev/principal-component-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**Input data matrix, centered.** Per our usual conventions, let $\\hat{x}_0, \\ldots, \\hat{x}_{m-1}$ be th $m$ data points, where each $x_i \\in \\mathbb{R}^d$ is a single observation. Each observation is represented by a $d$-dimensional real-valued vector corresponding to $d$ measured predictors. As usual, we can stack these into a data matrix, denoted $X \\equiv \\left(\\begin{array}{c} \\hat{x}_0^T \\\\ \\vdots \\\\ \\hat{x}_{m-1}^T \\end{array}\\right)$.\n",
    "\n",
    "However, we'll add one more important assumption: these data should be _centered_ about their mean, i.e., $\\frac{1}{m} \\sum_{i=0}^{m-1} \\hat{x}_i = 0$. If the observations are not centered initially, then preprocess them accordingly.\n",
    "\n",
    "**Projections.** Let $\\varphi \\in \\mathbb{R}^d$ be a vector of unit length, i.e., $\\|\\varphi\\|_2^2 = \\varphi^T \\varphi = 1$. The _projection_ of a data point $\\hat{x}_i$ onto $\\varphi$ is $\\hat{x}_i^T \\varphi$, which measures the length of the projected vector.\n",
    "\n",
    "Let $J(\\varphi)$ be a cost function that is proportional to the mean squared projections of the data onto $\\varphi$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  J(\\phi)\n",
    "    & \\equiv & \\displaystyle\n",
    "      \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\hat{x}_i^T \\varphi)^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The additional factor of \"1/2\" is for aesthetic reasons. (It cancels out later on.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also apply some algebra-fu to the right-hand side to put it into a more concise matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  J(\\phi)\n",
    "    &    =   & \\displaystyle\n",
    "      \\frac{1}{2} \\varphi^T \\left(\\frac{1}{m} \\sum_{i=0}^{m-1} \\hat{x}_i \\hat{x}_i^T \\right) \\varphi \\\\\n",
    "    &    =   & \\displaystyle\n",
    "      \\frac{1}{2} \\varphi^T \\left(\\frac{1}{m} X^T X\\right) \\varphi \\\\\n",
    "    & \\equiv & \\displaystyle\n",
    "      \\frac{1}{2} \\varphi^T C \\varphi.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In the last step, we defined $C \\equiv \\frac{1}{m} X^T X$; in statistics, $C$ is also known as the [_sample covariance matrix_](https://en.wikipedia.org/wiki/Sample_mean_and_covariance) of the data. (Recall that the data are centered about their mean.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal components via maximizing projections.** There are several ways to formulate the PCA problem. Here we consider the one based on _maximizing projections_.\n",
    "\n",
    "Start by defining a _principal component_ of the data $X$ to be a vector, $\\varphi$, of unit length that maximizes the sum of squared projections.\n",
    "\n",
    "To convert this definition into a formal problem, there is a technique known as the _method of Langrange multipliers_, which may be applied to any minimization or maximization problem that has equality constraints. The idea is to modify the cost function in a certain way that effectively incorporates each constraint: for each constraint you will add to the cost function a term proportional to a dummy parameter times some form of the constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh? It's easiest to see this formulation by example. In the case of a principal component, the modified cost function is\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{J}(\\varphi, \\lambda)\n",
    "    & \\equiv & \\displaystyle\n",
    "      J(\\varphi) + \\frac{\\lambda}{2} (1 - \\varphi^T \\varphi),\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where the second term captures the constraint: it introduces a dummy optimization parameter, $\\lambda$, times the constraint that $\\varphi$ has unit length, i.e., $\\|\\varphi\\|_2^2 = \\varphi^T \\varphi = 1$, or $1 - \\varphi^T \\varphi = 0$.\n",
    "\n",
    "> The reason to add the constraint in this way will become clear momentarily.\n",
    ">\n",
    "> As before, the factor of \"1/2\" is there solely for aesthetic reasons and will cancel out momentarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization task is then to find the $\\varphi_*$ and $\\lambda_*$ such that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  (\\varphi_*, \\lambda_*)\n",
    "    & \\equiv & \\displaystyle\n",
    "      \\underset{\\varphi, \\lambda}{\\arg\\max} \\, \\hat{J}(\\varphi, \\lambda).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find its solution, you just need to \"take derivatives\" of $\\hat{J}$ with respect to $\\varphi$ and $\\lambda$, and then set these derivatives to 0.\n",
    "\n",
    "**Exercise (optional).** Show that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\nabla_\\varphi \\hat{J} & = & C \\varphi - \\lambda \\varphi \\\\\n",
    "  \\displaystyle \\frac{\\partial}{\\partial \\lambda} \\hat{J} & = & \\frac{1}{2} (1 - \\varphi^T \\varphi).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Setting these to zero and solving yields the following computational problem:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  C \\varphi = X^T X \\varphi & = & \\lambda \\varphi \\\\\n",
    "  \\| \\varphi \\|_2^2 & = & 1.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> Is it now clear why the constraint was incorporated into $\\hat{J}$ as it was? Doing so produces a second equation that _exactly_ captures the constraint!\n",
    "\n",
    "This problem is an _eigenproblem_, which is the task of computing an eigenvalue and its corresponding eigenvector of $C = \\frac{1}{m} X^T X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $C$ will usually have many eigenvalues and eigenvectors. So which one do you want? Plug the eigenvector back into the original cost function. Then, $J(\\varphi) = \\frac{1}{2} \\varphi^T C \\varphi = \\frac{\\lambda}{2} \\varphi^T \\varphi = \\frac{\\lambda}{2}$. In other words, to maximize $J(\\varphi)$ you should pick the $\\varphi$ with the largest eigenvalue $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an eigenpair via the SVD\n",
    "\n",
    "So how do you find the eigenvectors of $C$? That is, what algorithm will compute them?\n",
    "\n",
    "One way is to form $C$ explicitly and then call an off-the-shelf eigensolver. However, forming $C$ explicitly from the data $X$ may be costly in time and storage, not to mention possibly less accurate. (Recall the condition number blow-up problem in the case of solving the normal equations.)\n",
    "\n",
    "Instead, we can turn to the \"Swiss Army knife\" of linear algebra, which is the _singular value decomposition_, or SVD. It is an extremely versatile tool for simplifying linear algebra problems. It can also be somewhat expensive to compute accurately, but a lot of scientific and engineering effort has gone into building robust and reasonably efficient SVD algorithms. So let's assume these exist -- and they do in both [Numpy](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) and [Scipy](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html) -- and use them accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The SVD.** Every real-valued matrix $X \\in \\mathbb{R}^{m \\times d}$ has a _singular value decomposition_. Let $k = \\min(m, d)$. The SVD of $X$ is the factorization, $X = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times k}$ and $V \\in \\mathbb{R}^{k \\times d}$ are orthogonal matrices ($U^T U = I$ and $V^T V = I$) and $\\Sigma$ is a $k \\times k$ diagonal matrix whose entries are $\\sigma_0 \\geq \\sigma_1 \\geq \\cdots \\geq \\sigma_{k-1} \\geq 0$.\n",
    "\n",
    "Usually, $m \\geq d$ so $k = d$. We'll assume that in what follows.\n",
    "\n",
    "Consider the columns of $U$ and $V$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  U & = & \\left(\\begin{array}{cccc} u_0 & u_1 & \\cdots & u_{d-1} \\end{array}\\right) \\\\\n",
    "  V & = & \\left(\\begin{array}{cccc} v_0 & v_1 & \\cdots & v_{d-1} \\end{array}\\right).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The columns of $U$ and of $V$ are also referred to as the _left singular vectors_ and _right singular vectors_, respectively.\n",
    "\n",
    "From these definitions, the SVD says that $X V = U \\Sigma$. This form is just a compact way of writing down a _system_ of independent vector equations,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  X v_i & = & \\sigma_i u_i.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in PCA, you care about $C = \\frac{1}{m} X^T X$. In terms of the SVD,\n",
    "\n",
    "$$X^T X = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T,$$\n",
    "\n",
    "or\n",
    "\n",
    "$$X^T X V = V \\Sigma^2.$$\n",
    "\n",
    "This relation may in turn be rewritten as the system of vector equations,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  X^T X v_i & = & \\sigma_i^2 v_i.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words, every pair $(\\varphi, \\lambda) \\equiv \\left(v_i, \\frac{\\sigma_i^2}{m}\\right)$ is a potential solution to the eigenproblem, $C\\varphi = \\frac{1}{m} X^T X \\varphi = \\lambda \\varphi$. The pair with the largest eigenvalue is $\\left(v_0, \\frac{\\sigma_0^2}{m}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank-$k$ approximations: the truncated SVD\n",
    "\n",
    "We motivated PCA by asking for a single vector $\\varphi$, which effectively projects the data onto a one-dimensional subspace (i.e., a line). You might instead want to represent the original $d$-dimensional data points on a $k$-dimensional surface or subspace, where $k \\leq d$. As the previous discussion suggests, you should probably choose the top-$k$ eigenvectors, $v_0, \\ldots, v_{k-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there is another \"principled\" reason for this choice.\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times d}$ be any matrix with an SVD given by $A = U \\Sigma V^T$. \n",
    "\n",
    "Let's furthermore define the _$k$-truncated SVD_, for any $k \\leq d$, as the $U_k \\Sigma_k V_k^T$, product where $U_k$, $\\Sigma_k$, and $V_k$ consist of the singular vectors and values corresponding to the $k$ largest singular values. That is, $U_k$ is the first $k$ columns of $U$, same for $V_k$, and $\\Sigma_k$ is the upper $k \\times k$ submatrix of $\\Sigma$.\n",
    "\n",
    "Now consider the following alternative way to write the SVD:\n",
    "\n",
    "$$\n",
    "  A = U \\Sigma V^T = \\sum_{i=0}^{d-1} u_i \\sigma_i v_i^T.\n",
    "$$\n",
    "\n",
    "Each term, $u_i \\sigma_i v_i^T$ is known as a _rank-$1$_ product. So the existence of the SVD means that $A$ may be written as a sum of rank-1 products.\n",
    "\n",
    "It would be natural to try to _approximate_ $A$ by truncating the SVD after $k$ terms, i.e.,\n",
    "\n",
    "$$\n",
    "  A \\approx U_k \\Sigma_k V_k^T = \\sum_{i=0}^{k-1} u_i \\sigma_i v_i^T.\n",
    "$$\n",
    "\n",
    "And in fact, there is _no_ rank-$k$ approximation of $A$ that is better than this one! In particular, consider _any_ pair of $k$ column vectors, $Y_k \\in \\mathbb{R}^{m \\times k}$ and $Z_k \\in \\mathbb{R}^{d \\times k}$; their product, $Y_k Z_k$ has rank at most $k$. Then there is a theorem that says the smallest difference between $A$ and the rank-$k$ product $Y_k Z_k$, measured in the Frobenius norm, is\n",
    "\n",
    "$$\n",
    "  \\min_{Y_k, Z_k} \\|A - Y_k Z_k^T\\|_F^2\n",
    "    = \\|A - U_k \\Sigma_k V_k^T\\|_F^2\n",
    "    = \\sigma_{k+1}^2 + \\sigma_{k+2}^2 + \\cdots + \\sigma_{d-1}^2.\n",
    "$$\n",
    "\n",
    "In other words, the truncated SVD gives the best rank-$k$ approximation to $A$ in the Frobenius norm. Moreover, the error of the approximation is the sum of the squares of all the smallest $d-k$ singular values.\n",
    "\n",
    "Applied to the covariance matrix, we may conclude that $C = \\frac{1}{m} X^T X \\approx \\frac{1}{m} V_k \\Sigma_k^2 V_k^T$ is in fact the best rank-$k$ approximation of $C$, which justifies choosing the $k$ eigenvectors corresponding to the top $k$ eigenvalues of $C$ as the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Solving PCA problem\n",
    "\n",
    "Based on the preceding discussion, here is the basic algorithm to compute the PCA, given the data $X$ and the desired dimension $k$ of the subspace.\n",
    "\n",
    "1. If the data are not already centered, transform them so that they have a mean of 0 in all coordinates, i.e., $\\displaystyle \\frac{1}{m} \\sum_{i=0}^{m-1} \\hat{x}_i = 0$.\n",
    "2. Compute the $k$-truncated SVD, $\\displaystyle X \\approx U_k \\Sigma_k V_k^T$.\n",
    "3. Choose $v_0, v_1, \\ldots, v_{k-1}$ to be the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
