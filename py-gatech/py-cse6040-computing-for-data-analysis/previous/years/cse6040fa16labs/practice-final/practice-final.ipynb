{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "# Important note!\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "YOUR_ID = \"\" # Please enter your GT login, e.g., \"rvuduc3\" or \"gtg911x\"\n",
    "COLLABORATORS = [] # list of strings of your collaborators' IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b11295002cc2b9549d6a2b01721b6701",
     "grade": true,
     "grade_id": "who__test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_CHECK_ID = re.compile (r'''[a-zA-Z]+\\d+|[gG][tT][gG]\\d+[a-zA-Z]''')\n",
    "assert RE_CHECK_ID.match (YOUR_ID) is not None\n",
    "\n",
    "collab_check = [RE_CHECK_ID.match (i) is not None for i in COLLABORATORS]\n",
    "assert all (collab_check)\n",
    "\n",
    "del collab_check\n",
    "del RE_CHECK_ID\n",
    "del re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter / IPython version check.** The following code cell verifies that you are using the correct version of Jupyter/IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2016: Practice Final Exam\n",
    "\n",
    "This notebook contains a set of practice questions that should help you review for the final exam. It is fairly close in form and content to what you should expect on the final.\n",
    "\n",
    "Note that this practice was designed for the Fall 2015 semester, which followed a slightly different syllabus. Therefore, it's possible we would not have covered the material you would need to know to complete these questions. Also, the testing code does not necessarily check whether your code is correct; rather, it may only produce diagnostic information, which you'd need to inspect manually for correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cells depend on an external module, [`cse6040utils.py`](https://raw.githubusercontent.com/rvuduc/cse6040-ipynbs/master/cse6040utils.py). You may want to peruse it to familiarize yourself with potentially useful helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import urllib\n",
    "import sqlite3 as db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cse6040utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Can you compute sample variance quickly _and_ accurately? ##\n",
    "\n",
    "Let $x_0, x_1, \\ldots, x_{n-1}$ be a sequence of $n$ observations in some experiment. The _sample mean_ is defined as,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mu_n & \\equiv & \\frac{1}{n} \\sum_{i=0}^{n-1} x_i.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The _sample variance_ is defined as,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\sigma_n^2 & \\equiv & \\frac{1}{n} \\sum_{i=0}^{n-1} (x_i - \\mu_n)^2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The standard way to compute the sample variance is to first compute the sample mean ($\\mu_n$), and then compute the sample variance ($\\sigma_n^2$). Thus, the standard approach requires _two passes_ through the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)** (2 points). Write a Python function to compute the sample mean. You may assume the input list, `x`, contains floating-point values as a Numpy array.\n",
    "\n",
    "> Ground rules: **_Do not_** use Numpy's `mean()` function. (You may use other Numpy functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a2b4ee6e01c695f22cac416f744cf628",
     "grade": false,
     "grade_id": "sample_mean",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_mean (x):\n",
    "    assert type (x) is np.ndarray\n",
    "    assert x.dtype == float\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c4ea4ada289df0809033629ef9950ee9",
     "grade": true,
     "grade_id": "sample_mean__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand (5)\n",
    "dx = sample_mean (x) - np.mean (x)\n",
    "\n",
    "print (\"Input:\", x)\n",
    "print (\"Your result:\", sample_mean (x))\n",
    "print (\"Error compared to Numpy's built-in function:\", dx)\n",
    "\n",
    "assert dx <= 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)** (2 points). Write a Python function to compute the sample variance, using the standard (\"two-pass\") approach. That is, your implementation should first compute $\\mu_n$ and use it to then complete the computation of $\\sigma_n^2$.\n",
    "\n",
    "> Ground rules: You can use `sample_mean()` above, but **_do not_** use Numpy's `mean()`, `var()`, or `std()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d0cd637a004efb47b6be51aed4d87208",
     "grade": false,
     "grade_id": "sample_variance",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_variance (x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4452058ce2256b757e1d40442481bb37",
     "grade": true,
     "grade_id": "sample_variance__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "delta_sigma_x = sample_variance (x) - np.var (x)\n",
    "print (\"Your result:\", sample_variance (x))\n",
    "print (\"Error compared to Numpy's built-in function:\", delta_sigma_x)\n",
    "assert delta_sigma_x <= 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (c)** (4 points). Suppose the dataset is very large and sweeping through it is, therefore, very slow. You might naturally ask whether there is a _one-pass_ approach for computing the sample variance. In other words, is it possible to compute the sample variance making just one sweep through the data?\n",
    "\n",
    "Your colleague suggests a one-pass method. The idea derives from the following observation.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\sigma_n^2\n",
    "    & \\equiv & \\frac{1}{n} \\sum_{i=0}^{n-1} (x_i - \\mu_n)^2 \\\\\n",
    "    &    =   & \\frac{1}{n} \\sum_{i=0}^{n-1} \\left( x_i^2 - 2 x_i \\mu_n + \\mu_n^2 \\right) \\\\\n",
    "    &    =   & \\left( \\frac{1}{n} \\sum_{i=0}^{n-1} x_i^2 \\right)\n",
    "               - 2 \\mu_n \\underbrace{\\left( \\frac{1}{n} \\sum_{i=0}^{n-1} x_i \\right)}_{= \\mu_n}\n",
    "               + \\mu_n^2 \\\\\n",
    "    &    =   & \\frac{1}{n} \\left( \\sum_{i=0}^{n-1} x_i^2 \\right) - \\mu_n^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus, in just one pass through the data, you could compute the pair, $(a, b)^T$ by the vector sum,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left(\\begin{array}{c}\n",
    "    a \\\\\n",
    "    b\n",
    "  \\end{array}\\right)\n",
    "    & \\leftarrow & \\sum_{i=0}^{n-1} \\left(\\begin{array}{c}\n",
    "                                      x_i \\\\\n",
    "                                      x_i^2\n",
    "                                    \\end{array}\\right), \\\\\n",
    "  v & \\leftarrow & \\frac{b}{n} - \\left (\\frac{a}{n} \\right)^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For this part of the exercise, implement this one-pass scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9dd6cd564fcca1488ecfdc410a256fac",
     "grade": false,
     "grade_id": "one_pass",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_variance__one_pass (x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2b60d115ec723a1cd19e2bb0e956502e",
     "grade": true,
     "grade_id": "one_pass__check",
     "locked": true,
     "points": 4,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing code -- Not graded, so you can modify this cell when debugging\n",
    "var0 = np.var (x)\n",
    "var1 = sample_variance (x)\n",
    "var2 = sample_variance__one_pass (x)\n",
    "\n",
    "print (\"Numpy's result:\", var0)\n",
    "print (\"Your two-pass algorithm:\", var1)\n",
    "print (\"Your one-pass algorithm:\", var2)\n",
    "\n",
    "assert var2 - var1 <= 1e-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (d)** (2 points). With respect to its _numerical accuracy_, the one-pass approach can have a downside. Explain what the problem can be.\n",
    "\n",
    "> As of November 9, 2016, we have not yet covered the material relevant to this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0b1688873dc91d048eabf209be4ce46f",
     "grade": true,
     "grade_id": "explain",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Linear Regression via Maximum Likelihood Estimation\n",
    "\n",
    "Suppose you are given a sequence of observations, $(x_0, y_0), (x_1, y_1), \\ldots, (x_{n-1}, y_{n-1})$, where $x_i$ and $y_i$ are real numbers. You decide to fit a _noisy line_ to these observations:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  y_i & = & \\theta_0 + \\theta_1 x_i + \\epsilon_i,\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\theta_0$ is a y-intercept, $\\theta_1$ is the slope of a line, and $\\epsilon_i$ is an unknown random variable intended to model the error or noise in the observation. In other words, you believe the underlying phenomenon should produce the value $\\theta_0 + \\theta_1 x_i$ given $x_i$, but each measurement instead produces a noisy value $\\theta_0 + \\theta_1 x_i + \\epsilon_i$, where $\\epsilon_i$ is drawn from some distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be written more compactly as $y = X\\theta + \\epsilon$ using our usual conventions,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  y & \\equiv &\n",
    "    (y_0, \\ldots, y_{n-1})^T \\\\\n",
    "  X & \\equiv &\n",
    "    \\left(\\begin{array}{cc} 1 & x_0 \\\\ \\vdots & \\vdots \\\\ 1 & x_{n-1} \\end{array}\\right) \\\\\n",
    "  \\theta & \\equiv &\n",
    "    (\\theta_0, \\theta_1)^T \\\\\n",
    "  \\epsilon & \\equiv & (\\epsilon_0, \\ldots, \\epsilon_{n-1})^T.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you further believe that each error $\\epsilon_i$ is independent and identically distributed as a normal (Gaussian) random variable, $\\mathcal{N}(0, \\sigma^2)$, with mean $0$ and variance $\\sigma^2$. In this case, given $x_i$ and the parameters $\\theta$ and $\\sigma$, then $y_i$ will also be normally distributed as $\\mathcal{N}(\\theta_0 + \\theta_1 x_i, \\sigma^2)$. Thus, the conditional probability of the observed value $y_i$ may be written as\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mbox{Pr}[y_i \\,|\\, x_i; \\theta, \\sigma]\n",
    "    & \\equiv & \\displaystyle\n",
    "      \\frac{1}{\\sigma \\sqrt{2 \\pi}}\n",
    "        \\exp \\left[-\\frac{(y_i - \\theta_0 - \\theta_1 x_i)^2}{2\\sigma^2}\\right].\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)** (2 points). Show that the log-likelihood of the parameters, $(\\theta, \\sigma)$, given the observations is\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle\n",
    "      - n \\ln \\left(\\sigma \\sqrt{2 \\pi}\\right) - \\frac{1}{2\\sigma^2} \\|y - X\\theta\\|_2^2.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)** (2 points). Show that the gradient of $\\mathcal{L}$ with respect to $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\nabla_\\theta \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle \\frac{X^T \\left(y - X \\theta\\right)}{\\sigma^2}.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (c)** (2 points). Show that the partial derivative of $\\mathcal{L}$ with respect to $\\sigma$ is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl} \\displaystyle\n",
    "  {\\frac{\\partial}\n",
    "       {\\partial \\sigma}}  \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle\n",
    "      -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\|y - X \\theta\\|_2^2.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (d)** (2 points). Suppose you determine the parameters that maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  (\\theta_*, \\sigma_*)\n",
    "    & = & \\arg\\max_{\\theta, \\sigma} \\, \\mathcal{L}(\\theta, \\sigma)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Show that these estimates are given by,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  X^T X \\theta_* & = & X^T y \\\\\n",
    "  \\sigma_*^2 & = & \\displaystyle \\frac{1}{n} \\|y - X\\theta_*\\|_2^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Observe that the MLE procedure, being based on a particular model, also yields more information about the data than the _ad hoc_ least squares minimization procedure, in the form of an estimate for $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (e)** (2 points). Write a Python function to determine $\\theta$ and $\\sigma$ from a set of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a83c2a85acc3c474402d0098c3f4315f",
     "grade": false,
     "grade_id": "linreg_mle",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linreg_mle (X, y):\n",
    "    assert type (y) is np.ndarray\n",
    "    assert type (X) is np.ndarray\n",
    "    assert len (y) == len (X)\n",
    "    \n",
    "    # Your code should compute and return the following:\n",
    "    theta = np.array ([[0],\n",
    "                       [0]])\n",
    "    sigma = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return (theta, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bcec4d9b21e191b92c053fbf8aec286c",
     "grade": true,
     "grade_id": "linreg_mle__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This code cell tests your implementation on a synthetic data set.\n",
    "# It compares your MLE-based procedure against conventional least squares.\n",
    "# It is not graded so you are free to modify it for testing and debugging.\n",
    "\n",
    "n = 50 # Number of observations\n",
    "theta_true = cse6040utils.generate_model (1)\n",
    "sigma_true = 0.1\n",
    "(X, y) = cse6040utils.generate_data (n, theta_true, sigma=sigma_true)\n",
    "\n",
    "print (X.shape)\n",
    "print (theta_true.shape)\n",
    "print (y.shape)\n",
    "\n",
    "print (\"\\nCondition number of the data matrix: \", np.linalg.cond (X))\n",
    "print (\"True model coefficients:\", theta_true.T, sigma_true)\n",
    "\n",
    "theta_lstsq = cse6040utils.linreg_fit_lstsq (X, y)\n",
    "(theta_mle, sigma_mle) = linreg_mle (X, y)\n",
    "\n",
    "print (\"\\nEstimated model coefficients via least squares:\", theta_lstsq.T)\n",
    "print (\"Relative error in the coefficients:\", cse6040utils.rel_diff (theta_lstsq, theta_true))\n",
    "\n",
    "print (\"\\nEstimated model coefficients via MLE:\", theta_mle.T, sigma_mle)\n",
    "print (\"Relative error in the coefficients:\", cse6040utils.rel_diff (theta_mle, theta_true))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot (X[:, 1], y, 'bo') # Noisy observations\n",
    "ax1.plot (X[:, 1], X.dot (theta_lstsq), 'r^') # Fit via least squares\n",
    "ax1.plot (X[:, 1], X.dot (theta_mle), 'gv')\n",
    "ax1.plot (X[:, 1], X.dot (theta_true), 'm*') # True solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: A linear model of exam scores\n",
    "\n",
    "In this problem, you'll try to try to predict a student's final exam score from his or her midterm scores, using your `linreg_mle()` procedure from Problem 2 (above).\n",
    "\n",
    "These data come from, _\"Test scores for general psychology\"_, available at: http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading these scores into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_psych = pd.read_csv ('./exam-scores.csv')\n",
    "display (df_psych.head ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regard this data set as having 3 predictors (`Exam1`, `Exam2`, and `Exam3`) and 1 response variable (`FinalExam`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)** (2 points). Use Seaborn's `pairplot()` routine to inspect how the predictors and response variables relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8b365947f54f6f5df5a65736ab0ae217",
     "grade": true,
     "grade_id": "pairplot",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Call sns.pairplot (...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)** (2 points). Use your `linreg_mle()` procedure to estimate the $\\theta_*$ and $\\sigma_*$ that fit these data. Simply print the results.\n",
    "\n",
    "To help you out, we've provided some code to build $X$ and $y$; see the two code cells below, and fill in the third code cell to compute $\\theta_*$ and $\\sigma_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_to_augmat (df):\n",
    "    \"\"\"\n",
    "    Converts a Pandas data frame, 'df', into an augmented data matrix,\n",
    "    that is, a matrix whose columns are taken from the columns of 'df'\n",
    "    with an additional column of all ones prepended to it.\n",
    "    \"\"\"\n",
    "    A = df.as_matrix ()\n",
    "    return np.insert (A, 0, 1.0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract response variable (final exam) and predictors (exams 1, 2, and 3)\n",
    "y_psych = df_psych[['FinalExam']].as_matrix ()\n",
    "X_psych = df_to_augmat (df_psych[['Exam1', 'Exam2', 'Exam3']])\n",
    "\n",
    "print (\"Data matrix (first few rows):\")\n",
    "print (X_psych[0:5, :])\n",
    "print (\"   ...\")\n",
    "\n",
    "print (\"\\nResponse variable (first few entries):\")\n",
    "print (y_psych[0:5, :])\n",
    "print (\"   ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "494e1f55ddfd4558c5499828c454bd7d",
     "grade": true,
     "grade_id": "linreg_mle__apply",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Linear regression via gradient descent\n",
    "\n",
    "In this problem, you'll continue Problem 2 by developing a _gradient descent_ method for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the log-likelihood for linear regression assuming $n$ i.i.d. samples and normally distributed errors (with mean 0 and variance $\\sigma^2$) has a log-likelihood given by,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle\n",
    "      - n \\ln \\left(\\sigma \\sqrt{2 \\pi}\\right) - \\frac{1}{2\\sigma^2} \\|y - X\\theta\\|_2^2,\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $X$ is the $n \\times (d+1)$ _augmented_ data matrix and $y$ the vector of observed response variable values.\n",
    "\n",
    "In the MLE procedure, your goal is to determine the $\\theta_*, \\sigma_*$ values that maximize $\\mathcal{L}(\\theta, \\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)** (4 points). Recall the gradient ascent/descent procedure. Complete the following incomplete functions, which help specialize the gradient descent procedure for the case of linear regression.\n",
    "\n",
    "(You might refer back to your results in Problem 2 and complete this implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "434136b835c7e3a530f77d497ca2908b",
     "grade": true,
     "grade_id": "linreg_grad_theta_loglikelihood",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linreg_grad_theta_loglikelihood (X, y, theta, sigma):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the linear regression log-likelihood\n",
    "    with respect to 'theta', evaluated at (X, y, theta, sigma).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "86cad14169e77bcb704af92351c76418",
     "grade": true,
     "grade_id": "linreg_grad_sigma_loglikelihood",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linreg_grad_sigma_loglikelihood (X, y, theta, sigma):\n",
    "    \"\"\"\n",
    "    Returns the partial derivative of the linear regression\n",
    "    log-likelihood with respect to 'sigma', evaluated at\n",
    "    the point (X, y, theta, sigma).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "**Part (b)** (4 points). The function below contains the \"main loop\" of gradient descent. Complete the function, in particular by updating the `theta_t` and `sigma_t` inside the loop.\n",
    "\n",
    "> This function is written to optionally save and return all of the `theta_t` and `sigma_t` estimates across iterations, for later inspection in _Part (c)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e648f78fc093edaa7352e00140617ff5",
     "grade": true,
     "grade_id": "linreg_mle_gd",
     "locked": false,
     "points": 4,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linreg_mle_gd (X, y, ALPHA=0.1, MAX_STEP=1000, history=False):\n",
    "    \"\"\"\n",
    "    Returns (theta, sigma), the parameters of a linear regression model\n",
    "    based on maximum likelihood estimation using a gradient descent\n",
    "    fitting procedure. The data are given by the matrix of predictors,\n",
    "    X, and the vector of responses, y.\n",
    "    \n",
    "    If 'history' is set to True, then this routine returns the\n",
    "    parameters at each iteration.\n",
    "    \"\"\"\n",
    "    n = X.shape[0] # No. of data points\n",
    "    d = X.shape[1] # Dimension (no. of features per data point)\n",
    "\n",
    "    # Initial guesses\n",
    "    theta_t = np.zeros ((d, 1))\n",
    "    sigma_t = 1.0\n",
    "    \n",
    "    if history:\n",
    "        theta_all = np.zeros ((d, MAX_STEP+1))\n",
    "        theta_all[:, 0:1] = theta_t\n",
    "        sigma_all = np.zeros (MAX_STEP+1)\n",
    "        sigma_all[0] = sigma_t\n",
    "\n",
    "    for t in range (MAX_STEP):\n",
    "        # @YOUSE: Fill in the code to update (theta_t, sigma_t)\n",
    "      \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Records your new (theta_t, sigma_t) values\n",
    "        if history:\n",
    "            theta_all[:, t+1:t+2] = theta_t\n",
    "            sigma_all[t+1] = sigma_t\n",
    "    \n",
    "    if history:\n",
    "        return (theta_all, sigma_all)\n",
    "    else:\n",
    "        return (theta_t, sigma_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tests your implementation of linreg_mle_gd. This code cell\n",
    "# is not graded, so you can modify it for testing and debugging.\n",
    "\n",
    "df_psych = pd.read_csv ('./exam-scores.csv')\n",
    "\n",
    "print (\"First few rows of the data:\")\n",
    "display (df_psych.head ())\n",
    "\n",
    "y_psych = df_psych[['FinalExam']].as_matrix ()\n",
    "X_psych = df_to_augmat (df_psych[['Exam1', 'Exam2', 'Exam3']])\n",
    "\n",
    "(thetas, sigmas) = linreg_mle_gd (X_psych, y_psych, history=True)\n",
    "theta_psych_mle_gd = thetas[:, -1:]\n",
    "sigma_psych_mle_gd = sigmas[-1]\n",
    "\n",
    "print (\"\\nEstimated parameters:\", theta_psych_mle_gd.T.flatten (), sigma_psych_mle_gd)\n",
    "\n",
    "y_psych_predicted_mle_gd = X_psych.dot (theta_psych_mle_gd)\n",
    "relerr_psych_mle_gd = np.divide (y_psych_predicted_mle_gd - y_psych, y_psych)\n",
    "mean_sq_relerr_psych_mle_gd = np.linalg.norm (relerr_psych_mle_gd, ord=2) / len (y_psych)\n",
    "print (\"\\nMean squared relative error:\", mean_sq_relerr_psych_mle_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to plot the magnitude of the parameter estimates versus iteration count. Does the magnitude eventuall converge, and if so, after how many iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame (np.arange (thetas.shape[1]), columns=['Iteration'])\n",
    "params = np.insert (thetas, thetas.shape[0], sigmas, axis=0)\n",
    "plot_df['norm(Params)'] = np.linalg.norm (params, ord=2, axis=0)\n",
    "\n",
    "display (plot_df.head ())\n",
    "print (\"   ...\")\n",
    "display (plot_df.tail ())\n",
    "sns.lmplot ('Iteration', 'norm(Params)', plot_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Linear regression via gradient descent, revisited: an SQL-based implementation\n",
    "\n",
    "In this problem, you will take your solution from the preceding question and write a version that can work when the data resides in a SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, suppose the exam scores are stored in an SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exams_db_filename = \"exam-scores-tuples.db\"\n",
    "exams_db = db.connect (exams_db_filename)\n",
    "pd.read_sql_query ('SELECT * FROM ExamScores LIMIT 10', exams_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a data matrix, $X$, stored as an SQL table in the database named `X`. In particular, the `X` table will be a set of tuples, (`I`, `J`, `V`), where each tuple corresponds to the values $(i, j, x_{ij})$ of the (logical or mathematical) data matrix $X$.\n",
    "\n",
    "To simplify things a little, let's assume that all $x_{ij}$ entries exist and are unique.\n",
    "\n",
    "> _Note._ Notice that this procedure also inserts a column of `1.0` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "exams_cursor = exams_db.cursor ()\n",
    "exams_cursor.execute ('DROP TABLE IF EXISTS X')\n",
    "\n",
    "exams_cursor.execute (\"\"\"\n",
    "  CREATE TABLE X AS\n",
    "    SELECT StudentId AS I, Assignment AS J, Score AS V\n",
    "      FROM ExamScores\n",
    "      WHERE Assignment IN ('Exam1', 'Exam2', 'Exam3')\n",
    "\"\"\")\n",
    "\n",
    "exams_cursor.execute (\"\"\"\n",
    "  INSERT INTO X\n",
    "    SELECT DISTINCT StudentId AS I, 'Intercept' AS J, 1.0 AS Value\n",
    "      FROM ExamScores\n",
    "\"\"\")\n",
    "exams_db.commit () # Saves your answer to the DB\n",
    "\n",
    "display (pd.read_sql_query ('SELECT * FROM X ORDER BY I LIMIT 10', exams_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let's create a table `Y` to hold the response variable, which is the final exam score. That is, `Y` will have columns (`I`, `V`), and each row will correspond to student $i$'s final exam score, $y_i$.\n",
    "\n",
    "(Again, assume all $y_i$ exist and each $i$ is unique.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exams_cursor.execute ('DROP TABLE IF EXISTS Y')\n",
    "\n",
    "exams_cursor.execute (\"\"\"\n",
    "  CREATE TABLE Y AS\n",
    "    SELECT StudentId AS I, Score AS V\n",
    "      FROM ExamScores\n",
    "      WHERE Assignment = 'FinalExam'\n",
    "\"\"\")\n",
    "\n",
    "exams_db.commit ()\n",
    "display (pd.read_sql_query ('SELECT * FROM Y LIMIT 5', exams_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given matrix and vector objects stored as database tables, you can do linear algebra operations using joins. For example, suppose you wish to compute the product, $z = X^T y$, which is a subexpression of the gradient of the log-likelihood. Using the same convention that an output vector, when stored as a table, has a column index `I` and value `V`, then you might run the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exams_cursor.execute ('DROP TABLE IF EXISTS Z')\n",
    "exams_cursor.execute (\"\"\"\n",
    "  CREATE TABLE Z AS\n",
    "    SELECT X.J AS I, SUM (X.V * Y.V) AS V\n",
    "      FROM X, Y\n",
    "      WHERE X.I = Y.I\n",
    "      GROUP BY X.J\n",
    "\"\"\")\n",
    "exams_db.commit ()\n",
    "\n",
    "display (pd.read_sql_query ('SELECT * FROM Z ORDER BY I LIMIT 5', exams_db))\n",
    "exams_cursor.execute ('DROP TABLE Z') # clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the preceding conventions for creating matrices and vectors as database tables, here is your task: use SQL to implement the gradient descent procedure for computing a linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by filling in the generic functions below, which perform basic linear algebra operations. To help you get started, we've created some for you.\n",
    "\n",
    "> By convention, if the computed result is a _vector_ or _matrix_, then the following functions create a table in the database with a given name to store that result.\n",
    ">\n",
    "> However, for _scalar_ functions, these functions typically return just the value itself rather than creating a table just to store the scalar.\n",
    ">\n",
    "> For instance, here is a function that queries a given matrix table in the database and returns the number of rows it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sql_get_mat_num_rows (X, db):\n",
    "    \"\"\"Returns the number of rows in X.\"\"\"\n",
    "    c = db.cursor ()\n",
    "    n = pd.read_sql_query (\"\"\"\n",
    "      SELECT COUNT (*) AS V\n",
    "        FROM (SELECT DISTINCT I FROM {X})\n",
    "    \"\"\".format (X=X), db).iloc[0]['V']\n",
    "    return n\n",
    "\n",
    "# Example: Call with the name of a matrix table\n",
    "print (\"The matrix X has\", sql_get_mat_num_rows ('X', exams_db), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some more \"freebie\" code, which simply encapsulates the matrix-vector product, $y \\leftarrow A^T \\cdot x$, we saw in the motivating example above as a standalone function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_mat_trans_dot_vec (y, A, x, db):\n",
    "    \"\"\"\n",
    "    Computes y = A^T * x.\n",
    "    \n",
    "    The name of the input matrix table is given by A and the name\n",
    "    of the input vector table is given by x. The name of the\n",
    "    desired output vector table is y; if y exists, it is replaced\n",
    "    (dropped and recreated).\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {y}'.format (y=y))\n",
    "    c.execute (\"\"\"\n",
    "      CREATE TABLE {y} AS\n",
    "        SELECT {A}.J AS I, SUM ({A}.V * {x}.V) AS V\n",
    "          FROM {A}, {x}\n",
    "          WHERE {A}.I = {x}.I\n",
    "          GROUP BY {A}.J\n",
    "    \"\"\".format (y=y, A=A, x=x))\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's repeat the above example of computing $z \\leftarrow X^T y$, where $X$ and $y$ are the predictors matrix (`X` table) and response vector (`y` table), respectively. Recall that the output $z$ is just a dummy vector, which we'll store in a dummy table (`Z`) and then promptly remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo of sql_mat_trans_dot_vec()\n",
    "sql_mat_trans_dot_vec ('Z', 'X', 'Y', exams_db)\n",
    "display (pd.read_sql_query ('SELECT * FROM Z ORDER BY I LIMIT 5', exams_db))\n",
    "exams_cursor.execute ('DROP TABLE Z') # clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (a)** (4 points). Complete the implementation for a matrix-vector product, $y \\leftarrow A \\cdot x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a7f1c105134c2ec164c99474cd1e6bed",
     "grade": true,
     "grade_id": "sql_matvec",
     "locked": false,
     "points": 4,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sql_mat_dot_vec (y, A, x, db):\n",
    "    \"\"\"Computes y = A*x.\"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {y}'.format (y=y))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (b)** (4 points). Complete the implementation of an _axpy_ operation (pronounced \"AX-pee\"), which computes $z \\leftarrow \\alpha \\cdot x + y$, where $\\alpha$ is a scalar and $x$, $y$, and $z$ are vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "584ac4c7299b52494053c090b21fa639",
     "grade": true,
     "grade_id": "sql_axpy",
     "locked": false,
     "points": 4,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sql_axpy (z, a, x, y, db):\n",
    "    \"\"\"\n",
    "    Computes z = a*x + y, where x, y, and z are vectors and\n",
    "    a is a scalar.\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {z}'.format (z=z))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "**Part (c)** (2 points). Complete the following function, which should _return_ $x^T x$, given a vector $x$.\n",
    "\n",
    "> By the convention mentioned above, since $x^T x$ is a scalar, this function should simply _return_ that scalar value, rather than creating a table just to hold the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2a5ffb7aa9815dd53e0f7f38e8ee6fc8",
     "grade": true,
     "grade_id": "two_norm_sq",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sql_get_vec_dot_self (x, db):\n",
    "    \"\"\"\n",
    "    Returns x^T * x.\n",
    "    \n",
    "    Note that this function should _not_ modify the database.\n",
    "    Instead, it should run a query to compute the dot\n",
    "    product and return that value as a scalar.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those complete, here are some more freebies.\n",
    "\n",
    "The next two functions have a special feature: they update tables _in-place_!\n",
    "\n",
    "For instance, `sql_scale_inplace()` takes an existing vector table and _scales_ all the values (column `V` in the table) by a given scalar value `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_scale_inplace (y, a, db):\n",
    "    \"\"\"\n",
    "    Computes y = a*y, that is, scaling all values of a matrix\n",
    "    or vector table object by the numerical factor a. Note\n",
    "    that this operation completes _in-place_.\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('UPDATE {y} SET V = V * ({a})'.format (y=y, a=a))\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_vec_update (y, x, db):\n",
    "    \"\"\"Computes y = y + x. This action occurs in-place.\"\"\"\n",
    "    c = db.cursor ()\n",
    "    query = \"\"\"\n",
    "      REPLACE INTO {y} (I, V)\n",
    "        SELECT {y}.I AS I, {y}.V + {x}.V AS V\n",
    "          FROM {y}, {x} WHERE {y}.I = {x}.I\n",
    "    \"\"\".format (y=y, x=x)\n",
    "    c.execute (query)\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the gradient descent procedure, you will need to create a fitted-parameters vector. In this problem, let's use a table to store this vector of parameter values, i.e., $\\theta$ and $\\sigma$.\n",
    "\n",
    "The parameters vector's indices will come from the column values of $X$. In addition, let's augment the parameter vector with an additional entry, called `Sigma_`, to hold the estimated value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sql_create_params (X, params, db):\n",
    "    \"\"\"\n",
    "    Creates an initial parameters vector. It has 1 entry for each\n",
    "    column of the table (matrix) X, plus an additional entry named\n",
    "    `Sigma_` (note trailing underscore).\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {params}'.format (params=params))\n",
    "    c.execute (\"\"\"\n",
    "        CREATE TABLE {params} (I TEXT PRIMARY KEY, V REAL)\n",
    "      \"\"\".format (params=params))\n",
    "    \n",
    "    # The initial parameter vector is set entirely to '0', except\n",
    "    # for sigma (named 'Sigma_'), which is set to '1.0'\n",
    "    c.execute (\"\"\"\n",
    "        INSERT INTO {params}\n",
    "          SELECT DISTINCT {X}.J AS I, 0.0 AS V FROM {X}\n",
    "      \"\"\".format (params=params, X=X))\n",
    "    c.execute (\"\"\"\n",
    "        INSERT INTO {params} VALUES ('Sigma_', 1.0)\n",
    "      \"\"\".format (params=params))\n",
    "    \n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo code for the above routine\n",
    "sql_create_params ('X', 'Theta', exams_db)\n",
    "pd.read_sql_query ('SELECT * FROM Theta', exams_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part (d)** (8 points). Now for the fun part: implement the gradient descent procedure in the code cell(s) below. If you need additional helper subroutines beyond the ones shown above, feel free to create additional code cells for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "04a2ee204f742946cbb3d1ef5dcb37be",
     "grade": true,
     "grade_id": "sql_grad_loglikelihood",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sql_grad_loglikelihood (g, X, y, params, db):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a160b7b546c6044faadeb361be067f2a",
     "grade": true,
     "grade_id": "linreg_mle_gd_sql",
     "locked": false,
     "points": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linreg_mle_gd_sql (X, y, params, db, ALPHA=0.1, MAX_STEP=1000):\n",
    "    \"\"\"\n",
    "    Uses gradient descent to do maximum likelihood estimation for\n",
    "    the linear regression problem. The input predictors are\n",
    "    stored in a data matrix as a table named X; the response is a\n",
    "    vector stored as a table named y. The output is a table named\n",
    "    params, consisting of the fitted parameters.\n",
    "    \n",
    "    For more info on the parameters vector, see `create_params()`.\n",
    "    \"\"\"\n",
    "    sql_create_params (X, params, db)\n",
    "    \n",
    "    for t in range (MAX_STEP):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs your procedure and print its results. How does it compare to the pure Python implementation you did in the other practice question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linreg_mle_gd_sql ('X', 'Y', 'Theta', exams_db, MAX_STEP=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Parameters estimated by your SQL-based code:\")\n",
    "display (pd.read_sql_query ('SELECT * FROM Theta', exams_db))\n",
    "\n",
    "print (\"Parameters estimated by the Python code:\", theta_psych.T, sigma_psych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "**Part (e)** (2 points). You probably found the SQL-based implementation to be much more tedious than the Python version. It can also be much slower. Explain why and under what circumstances one might nevertheless want an SQL-based implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "91ce647a1fea1c5e62da5f8372244719",
     "grade": true,
     "grade_id": "why",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exams_db.close () # This cell closes the database; only call it when you are all done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
