{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MLE for linear regression\n",
    "\n",
    "A more statistically principled way to perform linear regression is to use maximum likelihood estimation (MLE).\n",
    "\n",
    "**Notation.** Suppose we observe $m$ responses and measure $d$ predictors for each response. Let $y_i$ denote the $i$-th observed response ($0 \\leq i < m$), and let $\\{x_{ij}\\}$ denote the corresponding predictor values ($1 \\leq j \\leq d$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A model: Linear with Gaussian noise.** Suppose our model of the data is that the response is a linear function of the predictors, but that each observation is corrupted by noise. That is, the $i$-th response is a random variable, $Y_i$, equal to \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  Y_i & \\equiv & \\theta_0 + x_{i,1} \\theta_1 + x_{i,2} \\theta_2 + \\cdots + x_{i,d} \\theta_d + E_i \\\\\n",
    "      & = & \\hat{x}_i^T \\theta + E_i,\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\hat{x}_i^T \\equiv \\left(\\begin{array}{ccccc} 1.0 & x_{i,1} & x_{i,2} & \\cdots & x_{i,d} \\end{array}\\right)$ is our usual notation for the row-vector of predictors (augmented by 1.0) and $E_i$ is the random error. Let's further assume that all errors are independent and identically distributed with a Gaussian (normal) probability distribution having 0 mean and standard deviation $\\sigma$, i.e., $E_i \\sim \\mathcal{N}(0, \\sigma^2)$. Then, $Y_i$ will also be normally distributed as $Y_i \\sim \\mathcal{N}(\\hat{x}_i^T \\theta, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this model, we can write the probability that $Y_i$ equals the observed response $y_i$ as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[Y_i=y_i \\,|\\, \\hat{x}_i^T; \\theta, \\sigma]\n",
    "  & = & \\dfrac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot \\exp \\left[ - \\dfrac{\\left(y_i - \\hat{x}_i^T \\theta\\right)^2}{2 \\sigma^2} \\right],\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "by definition of a Gaussian random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log-likelihood.** The parameters we wish to estimate are $\\theta$ and $\\sigma$. To do so, let's try to apply MLE. Taking $\\mathrm{Pr}[Y_i=y_i \\,|\\, \\hat{x}_i^T; \\theta, \\sigma]$ as the likelihood of the $i$-th observation, the log-likelihood of all observations becomes\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta, \\sigma; y, X)\n",
    "  & = & \\ln \\prod_{i=0}^{m-1} \\mathrm{Pr}[Y_i=y_i \\,|\\, \\hat{x}_i^T; \\theta, \\sigma] \\\\\n",
    "  & = & \\sum_{i=0}^{m-1} \\ln \\left\\{ \\mathrm{Pr}[Y_i=y_i \\,|\\, \\hat{x}_i^T; \\theta, \\sigma] \\right\\} \\\\\n",
    "  & = & \\sum_{i=0}^{m-1} \\ln \\left\\{ \\dfrac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot \\exp \\left[ - \\dfrac{\\left(y_i - \\hat{x}_i^T \\theta\\right)^2}{2 \\sigma^2} \\right] \\right\\} \\\\\n",
    "  & = & \\sum_{i=0}^{m-1} \\left[ -\\ln \\left(\\sigma \\sqrt{2 \\pi}\\right) - \\dfrac{1}{2 \\sigma^2} \\left( y_i - \\hat{x}_i^T \\theta \\right)^2 \\right] \\\\\n",
    "  & = & - m \\ln \\left(\\sigma \\sqrt{2 \\pi}\\right) - \\dfrac{1}{2 \\sigma^2} \\sum_{i=0}^{m-1} \\left( y_i - \\hat{x}_i^T \\theta \\right)^2.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This formula can be written more compactly in linear algebraic notation as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta, \\sigma; y, X)\n",
    "  & = & - m \\ln \\left( \\sigma \\sqrt{2 \\pi} \\right)\n",
    "        - \\dfrac{1}{2 \\sigma^2} \\left\\| y - X\\theta \\right\\|_2^2.\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of the log-likelihood.** To maximize the log-likelihood, we will need to compute the gradient of $\\mathcal{L}$ with respect to $\\theta$ and $\\sigma$.\n",
    "\n",
    "Starting with $\\sigma$,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\dfrac{\\partial}{\\partial \\sigma} \\mathcal{L}(\\theta, \\sigma; y, X)\n",
    "  & = & - \\dfrac{m}{\\sigma} + \\dfrac{\\| y - X \\theta \\|_2^2}{\\sigma^3}\n",
    "  & = & - \\dfrac{1}{\\sigma} \\left( m + \\dfrac{\\| y - X \\theta \\|_2^2}{\\sigma^2} \\right).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Moving on to $\\theta$, note that the log-likelihood can also be written as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta, \\sigma; y, X)\n",
    "  & = & - m \\ln \\left( \\sigma \\sqrt{2 \\pi} \\right)\n",
    "        - \\dfrac{1}{2 \\sigma^2} \\left( y^T y - 2 \\theta^T X^T y + \\theta^T X^T X \\theta \\right).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\nabla_\\theta \\mathcal{L}(\\theta, \\sigma; y, X)\n",
    "  & = & \\dfrac{1}{\\sigma^2} \\left( X^T y - X^T X \\theta \\right) \\\\\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximizing the log-likelihood.** At the maximum, the gradient will be zero. In that case, the maximizing $\\theta_*$ occurs when\n",
    "\n",
    "$$\n",
    "  X^T X \\theta_* = X^T y.\n",
    "$$\n",
    "\n",
    "In other words, the optimal choice comes from solving the normal equations.\n",
    "\n",
    "Similarly, the maximizing $\\sigma_*$ occurs when\n",
    "\n",
    "$$\n",
    "  \\sigma_*^2 = \\dfrac{\\| y - X \\theta \\|_2^2}{m}.\n",
    "$$\n",
    "\n",
    "That is, the estimated variance $\\sigma_*^2$ is just the mean sum of squared residuals. A small value of $\\sigma_*$ indicates a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
