{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2015: Practice Exam\n",
    "\n",
    "This notebook contains a set of practice questions that should help you review for the final exam. It is fairly close in form and content to what you should expect on the final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells, which load modules you are likely to need. Also, be sure to download or pull the latest version of [`cse6040utils.py`](https://raw.githubusercontent.com/rvuduc/cse6040-ipynbs/master/cse6040utils.py) and quickly review it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import urllib\n",
    "import sqlite3 as db\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cse6040utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Can you compute the variance quickly _and_ accurately? ##\n",
    "\n",
    "Let $x_0, x_1, \\ldots, x_{n-1}$ be a sequence of $n$ observations in some experiment. The _sample mean_ is defined as,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mu_n & \\equiv & \\frac{1}{n} \\sum_{i=0}^{n-1} x_i.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The _sample variance_ is defined as,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\sigma_n^2 & \\equiv & \\frac{1}{n} \\sum_{i=0}^{n-1} (x_i - \\mu_n)^2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The standard way to compute the sample variance is to first compute the sample mean ($\\mu_n$), and then compute the sample variance ($\\sigma_n^2$). Thus, the standard approach requires _two passes_ through the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Part a)_** Write a Python function to compute the sample mean. You may assume the input list, `x`, contains floating-point values as a Numpy array.\n",
    "\n",
    "> Ground rules: **_Do not_** use Numpy's `mean()` function. (You may use other Numpy functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_mean (x):\n",
    "    \"\"\"Returns the sample mean of the Numpy array x.\"\"\"\n",
    "    assert type (x) is np.ndarray\n",
    "    assert x.dtype == float\n",
    "    \n",
    "    # @YOUSE: Complete this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing code -- Not graded, so you can modify this cell when debugging\n",
    "x = np.random.rand (5)\n",
    "\n",
    "print \"Input:\", x\n",
    "print \"Your result:\", sample_mean (x)\n",
    "print \"Error compared to Numpy's built-in function:\", sample_mean (x) - np.mean (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b)** Write a Python function to compute the sample variance, using the standard (\"two-pass\") approach. That is, your implementation should first compute $\\mu_n$ and use it to then complete the computation of $\\sigma_n^2$.\n",
    "\n",
    "> Ground rules: You can use `sample_mean()` above, but **_do not_** use Numpy's `mean()`, `var()`, or `std()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_variance (x):\n",
    "    \"\"\"Returns the sample variance of the Numpy array x.\"\"\"\n",
    "    assert type (x) is np.ndarray\n",
    "    assert x.dtype == float\n",
    "    \n",
    "    # @YOUSE: Complete this function\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing code -- Not graded, so you can modify this cell\n",
    "print \"Your result:\", sample_variance (x)\n",
    "print \"Error compared to Numpy's built-in function:\", sample_variance (x) - np.var (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part c)** Suppose the dataset is very large and sweeping through it is, therefore, very slow. You might naturally ask whether there is a _one-pass_ approach for computing the sample variance. In other words, is it possible to compute the sample variance making just one sweep through the data?\n",
    "\n",
    "Your colleague suggests a one-pass method. The idea derives from the following observation.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\sigma_n^2\n",
    "    & \\equiv & \\frac{1}{n} \\sum_{i=0}^{n-1} (x_i - \\mu_n)^2 \\\\\n",
    "    &    =   & \\frac{1}{n} \\sum_{i=0}^{n-1} \\left( x_i^2 - 2 x_i \\mu_n + \\mu_n^2 \\right) \\\\\n",
    "    &    =   & \\left( \\frac{1}{n} \\sum_{i=0}^{n-1} x_i^2 \\right)\n",
    "               - 2 \\mu_n \\underbrace{\\left( \\frac{1}{n} \\sum_{i=0}^{n-1} x_i \\right)}_{= \\mu_n}\n",
    "               + \\mu_n^2 \\\\\n",
    "    &    =   & \\frac{1}{n} \\left( \\sum_{i=0}^{n-1} x_i^2 \\right) - \\mu_n^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus, in just one pass through the data, you could compute the pair, $(a, b)^T$ by the vector sum,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left(\\begin{array}{c}\n",
    "    a \\\\\n",
    "    b\n",
    "  \\end{array}\\right)\n",
    "    & \\leftarrow & \\sum_{i=0}^{n-1} \\left(\\begin{array}{c}\n",
    "                                      x_i \\\\\n",
    "                                      x_i^2\n",
    "                                    \\end{array}\\right), \\\\\n",
    "  v & \\leftarrow & \\frac{b}{n} - \\left (\\frac{a}{n} \\right)^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For this part of the exercise, implement this one-pass scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_variance__one_pass (x):\n",
    "    # @YOUSE: Implement the one-pass scheme\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing code -- Not graded, so you can modify this cell when debugging\n",
    "print \"Numpy's result:\", np.var (x)\n",
    "print \"Your two-pass algorithm:\", sample_variance (x)\n",
    "print \"Your one-pass algorithm:\", sample_variance__one_pass (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part d)** With respect to its _numerical accuracy_, the one-pass approach can have a downside. Explain what the problem can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Enter response here)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Linear Regression via Maximum Likelihood Estimation\n",
    "\n",
    "Suppose you are given a sequence of observations, $(x_0, y_0), (x_1, y_1), \\ldots, (x_{n-1}, y_{n-1})$, where $x_i$ and $y_i$ are real numbers. You decide to fit a _noisy line_ to these observations:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  y_i & = & \\theta_0 + \\theta_1 x_i + \\epsilon_i,\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\theta_0$ is a y-intercept, $\\theta_1$ is the slope of a line, and $\\epsilon_i$ is an unknown random variable intended to model the error or noise in the observation. In other words, you believe the underlying phenomenon should produce the value $\\theta_0 + \\theta_1 x_i$ given $x_i$, but each measurement instead produces a noisy value $\\theta_0 + \\theta_1 x_i + \\epsilon_i$, where $\\epsilon_i$ is drawn from some distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be written more compactly as $y = X\\theta + \\epsilon$ using our usual conventions,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  y & \\equiv &\n",
    "    (y_0, \\ldots, y_{n-1})^T \\\\\n",
    "  X & \\equiv &\n",
    "    \\left(\\begin{array}{cc} 1 & x_0 \\\\ \\vdots & \\vdots \\\\ 1 & x_{n-1} \\end{array}\\right) \\\\\n",
    "  \\theta & \\equiv &\n",
    "    (\\theta_0, \\theta_1)^T \\\\\n",
    "  \\epsilon & \\equiv & (\\epsilon_0, \\ldots, \\epsilon_{n-1})^T.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you further believe that each error $\\epsilon_i$ is independent and identically distributed as a normal (Gaussian) random variable, $\\mathcal{N}(0, \\sigma^2)$, with mean $0$ and variance $\\sigma^2$. In this case, given $x_i$ and the parameters $\\theta$ and $\\sigma$, then $y_i$ will also be normally distributed as $\\mathcal{N}(\\theta_0 + \\theta_1 x_i, \\sigma^2)$. Thus, the conditional probability of the observed value $y_i$ may be written as\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mbox{Pr}[y_i \\,|\\, x_i; \\theta, \\sigma]\n",
    "    & \\equiv & \\displaystyle\n",
    "      \\frac{1}{\\sigma \\sqrt{2 \\pi}}\n",
    "        \\exp \\left[-\\frac{(y_i - \\theta_0 - \\theta_1 x_i)^2}{2\\sigma^2}\\right].\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part a)** Show that the log-likelihood of the parameters, $(\\theta, \\sigma)$, given the observations is\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle\n",
    "      - n \\ln \\left(\\sigma \\sqrt{2 \\pi}\\right) - \\frac{1}{2\\sigma^2} \\|y - X\\theta\\|_2^2.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b)** Show that the gradient of $\\mathcal{L}$ with respect to $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\nabla_\\theta \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle \\frac{X^T \\left(y - X \\theta\\right)}{\\sigma^2}.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part c)** Show that the partial derivative of $\\mathcal{L}$ with respect to $\\sigma$ is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl} \\displaystyle\n",
    "  {\\frac{\\partial}\n",
    "       {\\partial \\sigma}}  \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle\n",
    "      -\\frac{n}{\\sigma} + \\frac{1}{\\sigma^3} \\|y - X \\theta\\|_2^2.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part d)** Suppose you determine the parameters that maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  (\\theta_*, \\sigma_*)\n",
    "    & = & \\arg\\max_{\\theta, \\sigma} \\, \\mathcal{L}(\\theta, \\sigma)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Show that these estimates are given by,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  X^T X \\theta_* & = & X^T y \\\\\n",
    "  \\sigma_*^2 & = & \\displaystyle \\frac{1}{n} \\|y - X\\theta_*\\|_2^2.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Observe that the MLE procedure, being based on a particular model, also yields more information about the data than the _ad hoc_ least squares minimization procedure, in the form of an estimate for $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part e)** Write a Python function to determine $\\theta$ and $\\sigma$ from a set of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linreg_mle (X, y):\n",
    "    assert type (y) is np.ndarray\n",
    "    assert type (X) is np.ndarray\n",
    "    assert len (y) == len (X)\n",
    "    \n",
    "    # Your code should compute and return the following:\n",
    "    theta = np.array ([[0],\n",
    "                       [0]])\n",
    "    sigma = 0.0\n",
    "    \n",
    "    # @YOUSE: Fill in your code here to update or compute\n",
    "    # theta and sigma.\n",
    "    pass\n",
    "\n",
    "    return (theta, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code cell just tests your implementation on a synthetic data set.\n",
    "# It compares your MLE-based procedure against conventional least squares.\n",
    "# It is not graded so you are free to modify it for testing and debugging.\n",
    "\n",
    "n = 50 # Number of observations\n",
    "theta_true = cse6040utils.generate_model (1)\n",
    "sigma_true = 0.1\n",
    "(X, y) = cse6040utils.generate_data (n, theta_true, sigma=sigma_true)\n",
    "\n",
    "print X.shape\n",
    "print theta_true.shape\n",
    "print y.shape\n",
    "\n",
    "print \"\\nCondition number of the data matrix: \", np.linalg.cond (X)\n",
    "print \"True model coefficients:\", theta_true.T, sigma_true\n",
    "\n",
    "theta_lstsq = cse6040utils.linreg_fit_lstsq (X, y)\n",
    "(theta_mle, sigma_mle) = linreg_mle (X, y)\n",
    "\n",
    "print \"\\nEstimated model coefficients via least squares:\", theta_lstsq.T\n",
    "print \"Relative error in the coefficients:\", cse6040utils.rel_diff (theta_lstsq, theta_true)\n",
    "\n",
    "print \"\\nEstimated model coefficients via MLE:\", theta_mle.T, sigma_mle\n",
    "print \"Relative error in the coefficients:\", cse6040utils.rel_diff (theta_mle, theta_true)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot (X[:, 1], y, 'bo') # Noisy observations\n",
    "ax1.plot (X[:, 1], X.dot (theta_lstsq), 'r^') # Fit via least squares\n",
    "ax1.plot (X[:, 1], X.dot (theta_mle), 'gv')\n",
    "ax1.plot (X[:, 1], X.dot (theta_true), 'm*') # True solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: A linear model of exam scores\n",
    "\n",
    "In this problem, you'll try to try to predict a student's final exam score from his or her midterm scores, using your `linreg_mle()` procedure from Problem 2 (above).\n",
    "\n",
    "These data come from, _\"Test scores for general psychology\"_, available at: http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading these scores into a Pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_psych = pd.read_csv ('http://cse6040.gatech.edu/fa15/exam-scores.csv')\n",
    "display (df_psych.head ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regard this data set as having 3 predictors (`Exam1`, `Exam2`, and `Exam3`) and 1 response variable (`FinalExam`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part a)** Use Seaborn's `pairplot()` routine to inspect how the predictors and response variables relate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @YOUSE: Fill in your implementation here\n",
    "#sns.pairplot (...)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b)** Use your `linreg_mle()` procedure to estimate the $\\theta_*$ and $\\sigma_*$ that fit these data.\n",
    "\n",
    "To help you out, we've provided some code to build $X$ and $y$; see the two code cells below, and fill in the third code cell to compute $\\theta_*$ and $\\sigma_*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_to_augmat (df):\n",
    "    \"\"\"\n",
    "    Converts a Pandas data frame, 'df', into an augmented data matrix,\n",
    "    that is, a matrix whose columns are taken from the columns of 'df'\n",
    "    with an additional column of all ones prepended to it.\n",
    "    \"\"\"\n",
    "    A = df.as_matrix ()\n",
    "    return np.insert (A, 0, 1.0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract response variable (final exam) and predictors (exams 1, 2, and 3)\n",
    "y_psych = df_psych[['FinalExam']].as_matrix ()\n",
    "X_psych = df_to_augmat (df_psych[['Exam1', 'Exam2', 'Exam3']])\n",
    "\n",
    "print \"Data matrix (first few rows):\"\n",
    "print X_psych[0:5, :]\n",
    "print \"   ...\"\n",
    "\n",
    "print \"\\nResponse variable (first few entries):\"\n",
    "print y_psych[0:5, :]\n",
    "print \"   ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @YOUSE: Write some code here to use your `linreg_mle()`\n",
    "# routine to estimate theta and sigma.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Linear regression via gradient descent\n",
    "\n",
    "In this problem, you'll continue Problem 2 by developing a _gradient descent_ method for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the log-likelihood for linear regression assuming $n$ i.i.d. samples and normally distributed errors (with mean 0 and variance $\\sigma^2$) has a log-likelihood given by,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\mathcal{L}(\\theta, \\sigma)\n",
    "    & = & \\displaystyle\n",
    "      - n \\ln \\left(\\sigma \\sqrt{2 \\pi}\\right) - \\frac{1}{2\\sigma^2} \\|y - X\\theta\\|_2^2,\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $X$ is the $n \\times (d+1)$ _augmented_ data matrix and $y$ the vector of observed response variable values.\n",
    "\n",
    "In the MLE procedure, your goal is to determine the $\\theta_*, \\sigma_*$ values that maximize $\\mathcal{L}(\\theta, \\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, recall the generic gradient descent procedure discussed in [Lab 26](http://nbviewer.ipython.org/github/rvuduc/cse6040-ipynbs/blob/master/26--logreg-mle-numopt.ipynb). In the following code cells, we ask you to specialize the gradient descent procedure for the case of linear regression. Refer back to your results in Problem 2 and complete this implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part a)** Complete the following function, which should return the gradient with respect to $\\theta$ of the log-likelihood for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linreg_grad_theta_loglikelihood (X, y, theta, sigma):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the linear regression log-likelihood\n",
    "    with respect to 'theta', evaluated at (X, y, theta, sigma).\n",
    "    \"\"\"\n",
    "    # @YOUSE: Fill in this implementation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b)** Complete the following function, which should return the partial derivative with respect to $\\sigma$ of the log-likelihood for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linreg_grad_sigma_loglikelihood (X, y, theta, sigma):\n",
    "    \"\"\"\n",
    "    Returns the partial derivative of the linear regression\n",
    "    log-likelihood with respect to 'sigma', evaluated at\n",
    "    the point (X, y, theta, sigma).\n",
    "    \"\"\"\n",
    "    # @YOUSE: Fill in this implementation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part c)** The function below contains the \"main loop\" of gradient descent. Complete the function, in particular by updating the `theta_t` and `sigma_t` inside the loop.\n",
    "\n",
    "> This function is written to optionally save and return all of the `theta_t` and `sigma_t` estimates across iterations, for later inspection in _Part b)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linreg_mle_gd (X, y, ALPHA=0.1, MAX_STEP=1000, history=False):\n",
    "    \"\"\"\n",
    "    Returns (theta, sigma), the parameters of a linear regression model\n",
    "    based on maximum likelihood estimation using a gradient descent\n",
    "    fitting procedure. The data are given by the matrix of predictors,\n",
    "    X, and the vector of responses, y.\n",
    "    \n",
    "    If 'history' is set to True, then this routine returns the\n",
    "    parameters at each iteration.\n",
    "    \"\"\"\n",
    "    n = X.shape[0] # No. of data points\n",
    "    d = X.shape[1] # Dimension (no. of features per data point)\n",
    "\n",
    "    # Initial guesses\n",
    "    theta_t = np.zeros ((d, 1))\n",
    "    sigma_t = 1.0\n",
    "    \n",
    "    if history:\n",
    "        # Make space for history and record the initial guess\n",
    "        theta_all = np.zeros ((d, MAX_STEP+1))\n",
    "        theta_all[:, 0:1] = theta_t\n",
    "        sigma_all = np.zeros (MAX_STEP+1)\n",
    "        sigma_all[0] = sigma_t\n",
    "\n",
    "    for t in range (MAX_STEP):\n",
    "        # @YOUSE: Fill in the code to update (theta_t, sigma_t)\n",
    "        pass\n",
    "\n",
    "        # Records your new (theta_t, sigma_t) values\n",
    "        if history:\n",
    "            theta_all[:, t+1:t+2] = theta_t\n",
    "            sigma_all[t+1] = sigma_t\n",
    "    \n",
    "    if history:\n",
    "        return (theta_all, sigma_all)\n",
    "    else:\n",
    "        return (theta_t, sigma_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tests your implementation of linreg_mle_gd. This code cell\n",
    "# is not graded, so you can modify it for testing and debugging.\n",
    "\n",
    "df_psych = pd.read_csv ('http://cse6040.gatech.edu/fa15/exam-scores.csv')\n",
    "\n",
    "print \"First few rows of the data:\"\n",
    "display (df_psych.head ())\n",
    "\n",
    "y_psych = df_psych[['FinalExam']].as_matrix ()\n",
    "X_psych = df_to_augmat (df_psych[['Exam1', 'Exam2', 'Exam3']])\n",
    "\n",
    "(thetas, sigmas) = linreg_mle_gd (X_psych, y_psych, history=True)\n",
    "theta_psych_mle_gd = thetas[:, -1:]\n",
    "sigma_psych_mle_gd = sigmas[-1]\n",
    "\n",
    "print \"\\nEstimated parameters:\", theta_psych_mle_gd.T.flatten (), sigma_psych_mle_gd\n",
    "\n",
    "y_psych_predicted_mle_gd = X_psych.dot (theta_psych_mle_gd)\n",
    "relerr_psych_mle_gd = np.divide (y_psych_predicted_mle_gd - y_psych, y_psych)\n",
    "mean_sq_relerr_psych_mle_gd = np.linalg.norm (relerr_psych_mle_gd, ord=2) / len (y_psych)\n",
    "print \"\\nMean squared relative error:\", mean_sq_relerr_psych_mle_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part d)** Run the code below to plot the magnitude of the parameter estimates versus iteration count. Does the magnitude eventually converge, and if so, after how many iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame (np.arange (thetas.shape[1]), columns=['Iteration'])\n",
    "params = np.insert (thetas, thetas.shape[0], sigmas, axis=0)\n",
    "plot_df['norm(Params)'] = np.linalg.norm (params, ord=2, axis=0)\n",
    "\n",
    "display (plot_df.head ())\n",
    "print \"   ...\"\n",
    "display (plot_df.tail ())\n",
    "sns.lmplot ('Iteration', 'norm(Params)', plot_df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Linear regression via gradient descent, revisited: an SQL-based implementation\n",
    "\n",
    "In this problem, you will take your solution from the preceding question and write a version that can work when the data resides in a SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, suppose the exam scores are stored in an SQLite database. The following code cell downloads a prepared SQL database and saves it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exams_db_url = 'http://cse6040.gatech.edu/fa15/exam-scores-tuples.db'\n",
    "(exams_db_filename, _) = urllib.urlretrieve (exams_db_url)\n",
    "print \"Downloaded database to:\", exams_db_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exams_db = db.connect (exams_db_filename)\n",
    "pd.read_sql_query ('SELECT * FROM ExamScores LIMIT 10', exams_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by selecting the predictor variables (`Exam1`, `Exam2`, and `Exam3`) into a data matrix, $X$, augmented with a column of ones. Let's store this data matrix as a table, also called `X`. It will be a set of tuples, (`I`, `J`, `V`), where each tuple corresponds to the values $(i, j, x_{ij})$ of $X$.\n",
    "\n",
    "You may assume that all $x_{ij}$ entries exist and are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exams_cursor = exams_db.cursor ()\n",
    "exams_cursor.execute ('DROP TABLE IF EXISTS X')\n",
    "exams_cursor.execute (\"\"\"\n",
    "  CREATE TABLE X AS\n",
    "    SELECT StudentId AS I, Assignment AS J, Score AS V\n",
    "      FROM ExamScores\n",
    "      WHERE Assignment IN ('Exam1', 'Exam2', 'Exam3')\n",
    "\"\"\")\n",
    "exams_cursor.execute (\"\"\"\n",
    "  INSERT INTO X\n",
    "    SELECT DISTINCT StudentId AS I, 'Intercept' AS J, 1.0 AS Value\n",
    "      FROM ExamScores\n",
    "\"\"\")\n",
    "exams_db.commit ()\n",
    "\n",
    "display (pd.read_sql_query ('SELECT * FROM X ORDER BY I LIMIT 10', exams_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let's create a table `Y` to hold the response variable, which is the final exam score. That is, `Y` will have columns (`I`, `V`), and each row will correspond to student $i$'s final exam score, $y_i$.\n",
    "\n",
    "(Again, assume all $y_i$ exist and each $i$ is unique.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exams_cursor.execute ('DROP TABLE IF EXISTS Y')\n",
    "exams_cursor.execute (\"\"\"\n",
    "  CREATE TABLE Y AS\n",
    "    SELECT StudentId AS I, Score AS V\n",
    "      FROM ExamScores\n",
    "      WHERE Assignment = 'FinalExam'\n",
    "\"\"\")\n",
    "exams_db.commit ()\n",
    "\n",
    "display (pd.read_sql_query ('SELECT * FROM Y LIMIT 5', exams_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given matrix and vector objects stored as database tables, you can do linear algebra operations using joins. For example, suppose you wish to compute the product, $z = X^T y$, which is a subexpression of the gradient of the log-likelihood. Using the same convention that an output vector, when stored as a table, has a column index `I` and value `V`, then you might run the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exams_cursor.execute ('DROP TABLE IF EXISTS Z')\n",
    "exams_cursor.execute (\"\"\"\n",
    "  CREATE TABLE Z AS\n",
    "    SELECT X.J AS I, SUM (X.V * Y.V) AS V\n",
    "      FROM X, Y\n",
    "      WHERE X.I = Y.I\n",
    "      GROUP BY X.J\n",
    "\"\"\")\n",
    "exams_db.commit ()\n",
    "\n",
    "display (pd.read_sql_query ('SELECT * FROM Z ORDER BY I LIMIT 5', exams_db))\n",
    "exams_cursor.execute ('DROP TABLE Z') # clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the preceding conventions for creating matrices and vectors as database tables, here is your task: use SQL to implement the gradient descent procedure for computing a linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by filling in the generic functions below, which perform basic linear algebra operations. To help you get started, we've created some for you.\n",
    "\n",
    "> By convention, if the computed result is a _vector_ or _matrix_, then the following functions create a table in the database with a given name to store that result.\n",
    ">\n",
    "> However, for _scalar_ functions, these functions typically return just the value itself rather than creating a table just to store the scalar.\n",
    ">\n",
    "> For instance, here is a function that queries a given matrix table in the database and returns the number of rows it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sql_get_mat_num_rows (X, db):\n",
    "    \"\"\"Returns the number of rows in X.\"\"\"\n",
    "    c = db.cursor ()\n",
    "    n = pd.read_sql_query (\"\"\"\n",
    "      SELECT COUNT (*) AS V\n",
    "        FROM (SELECT DISTINCT I FROM {X})\n",
    "    \"\"\".format (X=X), db).iloc[0]['V']\n",
    "    return n\n",
    "\n",
    "# Example: Call with the name of a matrix table\n",
    "print \"The matrix X has\", sql_get_mat_num_rows ('X', exams_db), \"rows.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some more \"freebie\" code, which simply encapsulates the matrix-vector product, $y \\leftarrow A^T \\cdot x$, we saw in the motivating example above as a standalone function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_mat_trans_dot_vec (y, A, x, db):\n",
    "    \"\"\"\n",
    "    Computes y = A^T * x.\n",
    "    \n",
    "    The name of the input matrix table is given by A and the name\n",
    "    of the input vector table is given by x. The name of the\n",
    "    desired output vector table is y; if y exists, it is replaced\n",
    "    (dropped and recreated).\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {y}'.format (y=y))\n",
    "    c.execute (\"\"\"\n",
    "      CREATE TABLE {y} AS\n",
    "        SELECT {A}.J AS I, SUM ({A}.V * {x}.V) AS V\n",
    "          FROM {A}, {x}\n",
    "          WHERE {A}.I = {x}.I\n",
    "          GROUP BY {A}.J\n",
    "    \"\"\".format (y=y, A=A, x=x))\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, let's repeat the above example of computing $z \\leftarrow X^T y$, where $X$ and $y$ are the predictors matrix (`X` table) and response vector (`y` table), respectively. Recall that the output $z$ is just a dummy vector, which we'll store in a dummy table (`Z`) and then promptly remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo of sql_mat_trans_dot_vec()\n",
    "sql_mat_trans_dot_vec ('Z', 'X', 'Y', exams_db)\n",
    "display (pd.read_sql_query ('SELECT * FROM Z ORDER BY I LIMIT 5', exams_db))\n",
    "exams_cursor.execute ('DROP TABLE Z') # clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part a)** Complete the implementation for a matrix-vector product, $y \\leftarrow A \\cdot x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_mat_dot_vec (y, A, x, db):\n",
    "    \"\"\"Computes y = A*x.\"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {y}'.format (y=y))\n",
    "    \n",
    "    # @YOUSE: Fill in this implementation\n",
    "    pass\n",
    "\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part b)** Complete the implementation of an _axpy_ operation (pronounced \"ax-pee\"), which computes $z \\leftarrow \\alpha \\cdot x + y$, where $\\alpha$ is a scalar and $x$, $y$, and $z$ are vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sql_axpy (z, a, x, y, db):\n",
    "    \"\"\"\n",
    "    Computes z = a*x + y, where x, y, and z are vectors and\n",
    "    a is a scalar.\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {z}'.format (z=z))\n",
    "    \n",
    "    # @YOUSE: Fill in this implementation\n",
    "    pass\n",
    "\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part c)** Complete the following function, which should _return_ $x^T x$, given a vector $x$.\n",
    "\n",
    "> By the convention mentioned above, since $x^T x$ is a scalar, this function should simply _return_ that scalar value, rather than creating a table just to hold the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_get_vec_dot_self (x, db):\n",
    "    \"\"\"\n",
    "    Returns x^T * x.\n",
    "    \n",
    "    Note that this function does _not_ modify the database.\n",
    "    Instead, it should run a query to compute the dot\n",
    "    product and return that value as a scalar.\n",
    "    \"\"\"\n",
    "    # @YOUSE: Fill in this implementation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those complete, let's give you some more freebies.\n",
    "\n",
    "The next two functions have a special feature: they update tables _in-place_!\n",
    "\n",
    "For instance, `sql_scale_inplace()` takes an existing vector table and _scales_ all the values (column `V` in the table) by a given scalar value `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_scale_inplace (y, a, db):\n",
    "    \"\"\"\n",
    "    Computes y = a*y, that is, scaling all values of a matrix\n",
    "    or vector table object by the numerical factor a. Note\n",
    "    that this operation completes _in-place_.\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('UPDATE {y} SET V = V * ({a})'.format (y=y, a=a))\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sql_vec_update (y, x, db):\n",
    "    \"\"\"Computes y = y + x. This action occurs in-place.\"\"\"\n",
    "    c = db.cursor ()\n",
    "    query = \"\"\"\n",
    "      REPLACE INTO {y} (I, V)\n",
    "        SELECT {y}.I AS I, {y}.V + {x}.V AS V\n",
    "          FROM {y}, {x} WHERE {y}.I = {x}.I\n",
    "    \"\"\".format (y=y, x=x)\n",
    "    c.execute (query)\n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the gradient descent procedure, you will need to create a fitted-parameters vector. In this problem, let's use a table to store this vector of parameter values, i.e., $\\theta$ and $\\sigma$.\n",
    "\n",
    "The parameters vector's indices will come from the column values of $X$. In addition, let's augment the parameter vector with an additional entry, called `Sigma_`, to hold the estimated value of $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sql_create_params (X, params, db):\n",
    "    \"\"\"\n",
    "    Creates an initial parameters vector. It has 1 entry for each\n",
    "    column of the table (matrix) X, plus an additional entry named\n",
    "    `Sigma_` (note trailing underscore).\n",
    "    \"\"\"\n",
    "    c = db.cursor ()\n",
    "    c.execute ('DROP TABLE IF EXISTS {params}'.format (params=params))\n",
    "    c.execute (\"\"\"\n",
    "        CREATE TABLE {params} (I TEXT PRIMARY KEY, V REAL)\n",
    "      \"\"\".format (params=params))\n",
    "    \n",
    "    # The initial parameter vector is set entirely to '0', except\n",
    "    # for sigma (named 'Sigma_'), which is set to '1.0'\n",
    "    c.execute (\"\"\"\n",
    "        INSERT INTO {params}\n",
    "          SELECT DISTINCT {X}.J AS I, 0.0 AS V FROM {X}\n",
    "      \"\"\".format (params=params, X=X))\n",
    "    c.execute (\"\"\"\n",
    "        INSERT INTO {params} VALUES ('Sigma_', 1.0)\n",
    "      \"\"\".format (params=params))\n",
    "    \n",
    "    db.commit ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test code for the above routine\n",
    "sql_create_params ('X', 'Theta', exams_db)\n",
    "pd.read_sql_query ('SELECT * FROM Theta', exams_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part d)** Now for the fun part: we want _you_ to implement the gradient descent procedure in the code cell(s) below. If you need additional helper subroutines beyond the ones shown above, feel free to create additional code cells for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linreg_mle_gd_sql (X, y, params, db, ALPHA=0.1, MAX_STEP=1000):\n",
    "    \"\"\"\n",
    "    Uses gradient descent to do maximum likelihood estimation for\n",
    "    the linear regression problem. The input predictors are\n",
    "    stored in a data matrix as a table named X; the response is a\n",
    "    vector stored as a table named y. The output is a table named\n",
    "    params, consisting of the fitted parameters.\n",
    "    \n",
    "    For more info on the parameters vector, see `create_params()`.\n",
    "    \"\"\"\n",
    "    # @YOUSE: Implement this procedure\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells run your procedure and print its results. How does it compare to the pure Python implementation you did in the other practice question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg_mle_gd_sql ('X', 'Y', 'Theta', exams_db, MAX_STEP=1000)\n",
    "pd.read_sql_query ('SELECT * FROM Theta', exams_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exams_db.close () # Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part e)** You probably found the SQL-based implementation to be much more tedious than the Python version. Explain why and under what circumstances one might nevertheless want an SQL-based implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Enter your response here)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
