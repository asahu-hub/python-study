{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2015 [14]: PageRank (still cont'd)\n",
    "\n",
    "> This notebook is identical to [Lab 13](http://nbviewer.ipython.org/github/rvuduc/cse6040-ipynbs/blob/master/13--pagerank-partial-solns.ipynb), but with solutions provided for Part 1 and partial solutions for Part 2.\n",
    "\n",
    "In this notebook, you'll implement the [PageRank algorithm](http://ilpubs.stanford.edu:8090/422/) summarized in class. You'll test it on a real dataset (circa 2005) that consists of [political blogs](http://networkdata.ics.uci.edu/data/polblogs/) and their links among one another.\n",
    "\n",
    "Note that the presentation in class follows the matrix view of the algorithm. Cleve Moler (inventor of MATLAB) has a nice set of notes [here](https://www.mathworks.com/moler/exm/chapters/pagerank.pdf).\n",
    "\n",
    "For today's notebook, you'll need to download the following additional materials:\n",
    "* A `cse6040utils` module, which is a Python module containing some handy routines from previous classes: [link](https://raw.githubusercontent.com/rvuduc/cse6040-ipynbs/master/cse6040utils.py) (Note: This module is already part of the `git` repo for our notebooks if you are pulling from there.)\n",
    "* A SQLite version of the political blogs dataset: http://cse6040.gatech.edu/fa15/poliblogs.db (~ 611 KiB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Explore the Dataset\n",
    "\n",
    "Let's start by looking at the dataset, to get a feel for what it contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, one of you asked recently how to get the schema for a SQLite database when using Python. Here is some code adapted from a few ideas floating around on the web. Let's use these to inspect the tables available in the political blogs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "\n",
    "def get_table_names (conn):\n",
    "    assert type (conn) == db.Connection # Only works for sqlite3 DBs\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "    return pd.read_sql_query (query, conn)\n",
    "\n",
    "def print_schemas (conn, table_names=None, limit=0):\n",
    "    assert type (conn) == db.Connection # Only works for sqlite3 DBs\n",
    "    if table_names is None:\n",
    "        table_names = get_table_names (conn)\n",
    "    c = conn.cursor ()\n",
    "    query = \"PRAGMA TABLE_INFO ({table})\"\n",
    "    for name in table_names:\n",
    "        c.execute (query.format (table=name))\n",
    "        columns = c.fetchall ()\n",
    "        print (\"=== {table} ===\".format (table=name))\n",
    "        col_string = \"[{id}] {name} : {type}\"\n",
    "        for col in columns:\n",
    "            print (col_string.format (id=col[0],\n",
    "                                      name=col[1],\n",
    "                                      type=col[2]))\n",
    "        print (\"\\n\")\n",
    "\n",
    "conn = db.connect ('poliblogs.db')\n",
    "\n",
    "for name in get_table_names (conn)['name']:\n",
    "    print_schemas (conn, [name])\n",
    "    query = '''SELECT * FROM %s LIMIT 5''' % name\n",
    "    print (pd.read_sql_query (query, conn))\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Write a snippet of code to verify that the vertex IDs are _dense_ in some interval $[1, n]$. That is, there is a minimum value of $1$, some maximum value $n$, and _no_ missing values between $1$ and $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  SELECT MIN(Id) AS MinId,\n",
    "         MAX(Id) AS MaxId,\n",
    "         COUNT(DISTINCT Id) AS NumDistinctIds\n",
    "    FROM Vertices\n",
    "'''\n",
    "df = pd.read_sql_query (query, conn)\n",
    "print df\n",
    "assert df.MinId[0] == 1\n",
    "assert df.MaxId[0] == df.NumDistinctIds[0]\n",
    "print (\"\\n==> Verified: Vertex ids cover [1, %d] densely.\" \\\n",
    "       % df.NumDistinctIds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Make sure every edge has its end points in the vertex table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  SELECT {col} FROM Edges\n",
    "    WHERE {col} NOT IN (SELECT Id FROM Vertices)\n",
    "'''\n",
    "\n",
    "df_s = pd.read_sql_query (query.format (col='Source'), conn)\n",
    "print (df_s['Source'])\n",
    "\n",
    "df_t = pd.read_sql_query (query.format (col='Target'), conn)\n",
    "print (df_t['Target'])\n",
    "\n",
    "assert df_s['Source'].empty\n",
    "assert df_t['Target'].empty\n",
    "\n",
    "print (\"==> Verified: All source and target IDs are vertices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Determine which vertices have no incident edges. Store the number of such vertices in a variable, `num_solo_vertices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  SELECT Id, Url\n",
    "    FROM Vertices\n",
    "    WHERE (Id NOT IN (SELECT DISTINCT Source FROM Edges))\n",
    "          AND (Id NOT IN (SELECT DISTINCT Target FROM Edges))\n",
    "'''\n",
    "df_solo_vertices = pd.read_sql_query (query, conn)\n",
    "print df_solo_vertices.head ()\n",
    "\n",
    "num_solo_vertices = len (df_solo_vertices)\n",
    "\n",
    "# Our testing code follows, assuming your `num_solo_vertices` variable:\n",
    "print (\"\\n==> %d vertices have no incident edges.\" % num_solo_vertices)\n",
    "assert num_solo_vertices == 266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Compute a view called `Outdegrees`, which contains the following columns:\n",
    "\n",
    "1. `Id`: vertex ID\n",
    "2. `Degree`: the out-degree of this vertex.\n",
    "\n",
    "To help you test your view, the following snippet includes a second query that selects from your view but adds a Url field and orders the results in descending order of degree. It also prints first few and last few rows of this query, so you can inspect the URLs as a sanity check. (Perhaps it also provides a small bit of entertainment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Complete this query:\n",
    "query = '''\n",
    "  CREATE VIEW IF NOT EXISTS Outdegrees AS\n",
    "    SELECT Source AS Id, COUNT(*) AS Degree\n",
    "      FROM Edges\n",
    "      GROUP BY Source\n",
    "'''\n",
    "c = conn.cursor ()\n",
    "c.execute (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "query = '''\n",
    "  SELECT Outdegrees.Id, Degree, Url\n",
    "    FROM Outdegrees, Vertices\n",
    "    WHERE Outdegrees.Id = Vertices.Id\n",
    "    ORDER BY -Degree\n",
    "'''\n",
    "df_outdegrees = pd.read_sql_query (query, conn)\n",
    "print \"==> A few entries with large out-degrees:\"\n",
    "display (df_outdegrees.head ())\n",
    "print \"\\n==> A few entries with small out-degrees:\"\n",
    "display (df_outdegrees.tail ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Query the database to extract a report of which URLs point to which URLs. Also include the source vertex out-degree and order the rows in descending order by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "  SELECT S.Url, T.Url, Out.Degree\n",
    "    FROM Edges AS E,\n",
    "         (SELECT Id, Url FROM Vertices) AS S,\n",
    "         (SELECT Id, Url FROM Vertices) AS T,\n",
    "         (SELECT Id, Degree FROM Outdegrees) AS Out\n",
    "    WHERE (E.Source=S.Id) AND (E.Target=T.Id) AND (E.Source=Out.Id)\n",
    "    ORDER BY -Out.Degree\n",
    "'''\n",
    "df_G = pd.read_sql_query (query, conn)\n",
    "\n",
    "from IPython.display import display\n",
    "display (df_G.head ())\n",
    "print (\"...\")\n",
    "display (df_G.tail ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement PageRank\n",
    "\n",
    "The following exercises will walk you through a possible implementation of PageRank for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Build a sparse matrix, `A_1`, that stores $G^TD^{-1}$, where $G^T$ is the transpose of the connectivity matrix $G$, and $D^{-1}$ is the diagonal matrix of inverse out-degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cse6040utils import sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract entries from the table\n",
    "query = '''\n",
    "  SELECT Target AS Row, Source AS Col, 1.0/Degree AS Val\n",
    "    FROM Edges, Outdegrees\n",
    "    WHERE Edges.Source = Outdegrees.Id\n",
    "'''\n",
    "df_A = pd.read_sql_query (query, conn)\n",
    "display (df_A.head (10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy entries from df_A into A_1\n",
    "A_1 = sparse_matrix () # Initially all zeros, with no rows or columns\n",
    "for (i, j, a_ij) in zip (df_A['Row'], df_A['Col'], df_A['Val']):\n",
    "    A_1[i-1][j-1] += a_ij # \"-1\" switches to 0-based indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Errata: Bug in matrix construction.** Based on questions from students after class, it seems the construction of $A \\equiv G^TD^{-1}$ as Prof. Vuduc described it in class has a subtle bug: it does _not_ treat unlinked pages correctly!\n",
    "\n",
    "To see why, suppose you are the random surfer visiting page $i$, and, with probability $\\alpha$, you decide to follow an outgoing link. But what if the page has no outgoing link?\n",
    "\n",
    "This scenario corresponds to row $i$ of $G$ being entirely zero. So, the random surfer would just \"disappear.\" The easiest fix to the model to account for this case is to assume that the random surfer stays on the same page, which means we should set  $a_{ii}$ to 1. The following code snippet handles this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select all vertices with no outgoing edges\n",
    "query = '''\n",
    "  SELECT Id FROM Vertices\n",
    "    WHERE Id NOT IN (SELECT DISTINCT Source FROM Edges)\n",
    "'''\n",
    "df_anti_social = pd.read_sql_query (query, conn)\n",
    "print (\"==> Found %d vertices with no outgoing links.\" \\\n",
    "       % len (df_anti_social))\n",
    "\n",
    "# Add self-edges for empty rows/columns\n",
    "for i in df_anti_social['Id']:\n",
    "    A_1[i-1][i-1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Implement a function to multiply a sparse matrix by a dense vector, assuming a dense vector defined as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_vector (n, init_val=0.0):\n",
    "    \"\"\"\n",
    "    Returns a dense vector of length `n`, with all entries set to\n",
    "    `init_val`.\n",
    "    \"\"\"\n",
    "    return [init_val] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spmv (n, A, x):\n",
    "    \"\"\"Returns a dense vector y of length n, where y = A*x.\"\"\"\n",
    "    y = dense_vector (n)\n",
    "    for (i, A_i) in A.items ():\n",
    "        s = 0\n",
    "        for (j, a_ij) in A_i.items ():\n",
    "            s += a_ij * x[j]\n",
    "        y[i] = s\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick test, let's verify that multiplying $A_1$ by the vector of all ones, $u$, counts the number of vertices.\n",
    "\n",
    "> Why should that be the case? Two of you asked about this after class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = df.NumDistinctIds[0] # Number of vertices, from Part 1\n",
    "u = dense_vector (n, 1.0)\n",
    "y = spmv (n, A_1, u)\n",
    "print sum (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Complete the PageRank implementation for this dataset. To keep it simple, you may take $\\alpha=0.85$, $x(0)$ equal to the vector of all $1/n$ values, and 25 iterations.\n",
    "\n",
    "Additionally, you may find the following functions helpful.\n",
    "\n",
    "> The support code in the next code cell differs _slightly_ from the notebook we posted originally. It renames those functions and provides additional functions (e.g., `vec_2norm`), in case you want to implement a residual-based termination test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper functions, in case you need them\n",
    "\n",
    "import math\n",
    "\n",
    "def vec_scale (x, alpha):\n",
    "    \"\"\"Scales the vector x by a constant alpha.\"\"\"\n",
    "    return [x_i*alpha for x_i in x]\n",
    "\n",
    "def vec_add_scalar (x, c):\n",
    "    \"\"\"Adds the scalar value c to every element of x.\"\"\"\n",
    "    return [x_i+c for x_i in x]\n",
    "\n",
    "def vec_sub (x, y):\n",
    "    \"\"\"Returns x - y\"\"\"\n",
    "    return [x_i - y_i for (x_i, y_i) in zip (x, y)]\n",
    "\n",
    "def vec_2norm (x):\n",
    "    \"\"\"Returns ||x||_2\"\"\"\n",
    "    return math.sqrt (sum ([x_i**2 for x_i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES BELOW. We've provided some scaffolding code,\n",
    "# so you just need to complete it.\n",
    "\n",
    "ALPHA = 0.85 # Probability of following some link\n",
    "MAX_ITERS = 25\n",
    "n = df.NumDistinctIds[0] # Number of vertices, from Part 1\n",
    "\n",
    "# Let X[t] store the dense x(t) vector at time t\n",
    "X = []\n",
    "\n",
    "x_0 = dense_vector (n, 1.0/n) # Initial distribution: 1/n at each page\n",
    "X.append (x_0)\n",
    "\n",
    "for t in range (1, MAX_ITERS):\n",
    "    # Complete this implementation\n",
    "    \n",
    "    X.append (...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Check your result by first inserting the _final_ computed PageRank vector back into the database, and then using a SQL query to see the ranked URLs. In your query output, also include _both_ the in-degrees and out-degrees of each vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write some code here to create a table in the database\n",
    "# called PageRank. It should have one column to hold the\n",
    "# page (vertex) ID, and one for the rank value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some helper code to compute a view containing the indegrees.\n",
    "\n",
    "query = '''\n",
    "  CREATE VIEW IF NOT EXISTS Indegrees AS\n",
    "    SELECT Target AS Id, COUNT(*) AS Degree\n",
    "      FROM Edges\n",
    "      GROUP BY Target\n",
    "'''\n",
    "c = conn.cursor ()\n",
    "c.execute (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Complete this query:\n",
    "query = '''\n",
    "  ...\n",
    "'''\n",
    "df_ranks = pd.read_sql_query (query, conn)\n",
    "display (df_ranks)\n",
    "sum (df_ranks['Rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** The `Vertices` table includes a column called, `Leaning`, which expresses a political leaning -- either \"Left\" or \"Right\". How might you use this column to come up with an alternative ranking scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise (advanced?).** Create an SQL-based implementation of the PageRank algorithm, where you implement the sparse matrix-vector multiply in SQL, rather than in Python as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
