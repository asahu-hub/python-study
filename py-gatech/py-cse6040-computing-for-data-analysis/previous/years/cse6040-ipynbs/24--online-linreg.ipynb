{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2015 [24]: \"Online\" regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook continues the linear regression problem from last time, but asks about a method that can estimate the regression coefficients when you only get to see samples \"one-at-a-time.\" We refer to such a fitting procedure as being \"online\" (or \"incrementally\"), rather than \"offline\" (or \"batched\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability with the problem size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, here is some code to help generate synthetic problems of a certain size, namely, $m \\times (d+1)$, where $m$ is the number of observations and $d$ the number of predictors. The $+1$ comes from our usual dummy coefficient for a non-zero intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_model (d):\n",
    "    \"\"\"Returns a set of (random) d+1 linear model coefficients.\"\"\"\n",
    "    return np.random.rand (d+1, 1)\n",
    "\n",
    "def generate_data (m, x, sigma=1.0/(2**0.5)):\n",
    "    \"\"\"\n",
    "    Generates 'm' noisy observations for a linear model whose\n",
    "    predictor (non-intercept) coefficients are given in 'x'.\n",
    "    Decrease 'sigma' to decrease the amount of noise.\n",
    "    \"\"\"\n",
    "    assert (type (x) is np.ndarray) and (x.ndim == 2) and (x.shape[1] == 1)\n",
    "    n = len (x)\n",
    "    A = np.random.rand (m, n)\n",
    "    A[:, 0] = 1.0\n",
    "    b = A.dot (x) + sigma*np.random.randn (m, 1)\n",
    "    return (A, b)\n",
    "\n",
    "def estimate_coeffs (A, b):\n",
    "    \"\"\"\n",
    "    Solves Ax=b by a linear least squares method.\n",
    "    \"\"\"\n",
    "    result = np.linalg.lstsq (A, b)\n",
    "    x = result[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_diff (x, y, ord=2):\n",
    "    \"\"\"\n",
    "    Computes ||x-y|| / ||y||. Uses 2-norm by default;\n",
    "    override by setting 'ord'.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm (x - y, ord=ord) / np.linalg.norm (y, ord=ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo the above routines for a 2-D dataset.\n",
    "\n",
    "m = 50\n",
    "x_true = generate_model (1)\n",
    "(A, b) = generate_data (m, x_true, sigma=0.1)\n",
    "\n",
    "print A.shape\n",
    "print x_true.shape\n",
    "print b.shape\n",
    "\n",
    "print \"Condition number of the data matrix: \", np.linalg.cond (A)\n",
    "print \"True model coefficients:\", x_true.T\n",
    "\n",
    "x = estimate_coeffs (A, b)\n",
    "\n",
    "print \"Estimated model coefficients:\", x.T\n",
    "print \"Relative error in the coefficients:\", rel_diff (x, x_true)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot (A[:, 1], b, 'b+') # Noisy observations\n",
    "ax1.plot (A[:, 1], A.dot (x), 'r*') # Fit\n",
    "ax1.plot (A[:, 1], A.dot (x_true), 'go') # True solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmark varying $m$\n",
    "\n",
    "Let's benchmark the time to compute $x$ when the dimension $n$ of each point is fixed but the number $m$ of points varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Benchmark, as 'm' varies:\n",
    "\n",
    "n = 32 # dimension\n",
    "M = [100, 1000, 10000, 100000, 1000000]\n",
    "times = [0.] * len (M)\n",
    "for (i, m) in enumerate (M):\n",
    "    x_true = generate_model (n)\n",
    "    (A, b) = generate_data (m, x_true, sigma=0.1)\n",
    "    t = %timeit -o estimate_coeffs (A, b)\n",
    "    times[i] = t.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** How does the running time scale with $m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_linear = [times[0]/M[0]*m for m in M]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.loglog (M, times, 'bo')\n",
    "ax1.loglog (M, t_linear, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Now fix the number $m$ of observations but vary the dimension $n$. How does time scale with $n$? Complete the benchmark code below to find out. In particular, given the array `N[:]`, compute an array, `times[:]`, such that `times[i]` is the running time for a problem of size `m`$\\times$`(N[i]+1)`.\n",
    "\n",
    "> Hint: You can basically copy and modify the preceding benchmark. Also, note that the code cell following the one immediately below will plot your results against $\\mathcal{O}(n)$ and $\\mathcal{O}(n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072]\n",
    "m = 5000\n",
    "times = [0.] * len (N)\n",
    "\n",
    "# @YOUSE: Implement a benchmark to compute the time,\n",
    "# `times[i]`, to execute a problem of size `N[i]`.\n",
    "for (i, n) in enumerate (N):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_linear = [times[0]/N[0]*n for n in N]\n",
    "t_quadratic = [times[0]/N[0]/N[0]*n*n for n in N]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.loglog (N, times, 'bo')\n",
    "ax1.loglog (N, t_linear, 'r--')\n",
    "ax1.loglog (N, t_quadratic, 'g--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An online algorithm\n",
    "\n",
    "The empirical scaling appears to be pretty good, being roughly linear in $m$ or at worst quadratic in $n$. But there is still a downside in time and storage: each time there is a change in the data, you appear to need to form the data matrix all over again and recompute the solution from scratch, possibly touching the entire data set again!\n",
    "\n",
    "This approach, which requires the full data, is often referred to as a _batched_ or _offline_ procedure.\n",
    "\n",
    "This begs the question, is there a way to incrementally update the model coefficients whenever a new data point, or perhaps a small batch of new data points, arrives? Such a procedure would be considered _incremental_ or _online_, rather than batched or offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: Key assumptions and main goal\n",
    "\n",
    "In the discussion that follows, assume that you only get to see the observations _one-at-a-time_. Let $(b_k, a_k^T)$ denote the current observation. (Relative to our previous notation, this tuple is just element $k$ of $b$ and row $k$ of $A$.)\n",
    "\n",
    "Additionally, assume that, at the time the $k$-th observation arrives, you start with a current estimate of the parameters, $\\hat{x}_k$, which is a vector. If for whatever reason you need to refer to element $i$ of that vector, use $\\hat{x}_{i,k}$. You will then compute a new estimate, $\\hat{x}_{k+1}$ using $\\hat{x}_k$ and $(b_k, a_k^T)$. For the discussion below, further assume that you throw out $\\hat{x}_k$ once you have $\\hat{x}_{k+1}$.\n",
    "\n",
    "As for your goal, recall that in the batch setting you start with _all_ the observations, $(b, A)$. From this starting point, you may estimate the linear regression model's parameters, $x$, by solving $Ax=b$. In the online setting, you compute estimates one at a time. After seeing all $m$ observations in $A$, your goal is to compute an $\\hat{x}_{m-1} \\approx x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An initial idea\n",
    "\n",
    "Indeed, there is a technique from the signal processing literature that we can apply to the linear regression problem, known as the least mean square (LMS) algorithm. Before describing it, let's start with an initial idea.\n",
    "\n",
    "Suppose that you have a current estimate of the parameters, $x_k$, when you get a new sample, $(b_k, a_k^T)$. The error in your prediction will be,\n",
    "\n",
    "$$b_k - a_k^T \\hat{x}_k.$$\n",
    "\n",
    "Ideally, this error would be zero. So, let's ask if there exists a _correction_, $\\Delta_k$, such that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "     & b_k - a_k^T (\\hat{x}_k + \\Delta_k) & = & 0 \\\\\n",
    "\\iff &              b_k - a_k^T \\hat{x}_k & = & a_k^T \\Delta_k\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Then, you could compute a new estimate of the parameter by $\\hat{x}_{k+1} = \\hat{x}_k + \\Delta_k$.\n",
    "\n",
    "This idea has a major flaw, which we will discuss below. But before we do, please try the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Verify that the following choice of $\\Delta_k$ would make the preceding equation true.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\Delta_k & = & \\frac{a_k}{\\|a_k\\|_2^2} (b_k - a_k^T \\hat{x}_k).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refining (or rather, \"hacking\") the basic idea: The least mean square (LMS) procedure\n",
    "\n",
    "The basic idea sketched above has at least one major flaw: the choice of $\\Delta_k$ might allow you to correctly predicts $b_k$ from $a_k$ and the new estimate $\\hat{x}_{k+1} = \\hat{x}_k + \\Delta_k$, but there is no guarantee that this new estimate $\\hat{x}_{k+1}$ preserves the quality of predictions made at all previous iterations!\n",
    "\n",
    "There are a number of ways to deal with this problem, which includes carrying out an update with respect to some (or all) previous data. However, there is also a simpler \"hack\" that, though it might require some parameter tuning, can be made to work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That hack is as follows. Rather than using $\\Delta_k$ as computed above, let's compute a different update that has a \"fudge\" factor, $\\phi$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "  &\n",
    "  \\hat{x}_{k+1} & = & \\hat{x}_k + \\Delta_k\n",
    "  \\\\\n",
    "  \\mbox{where}\n",
    "  &\n",
    "  \\Delta_k & = & \\phi \\cdot a_k (b_k - a_k^T \\hat{x}_k).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big question is how to choose $\\phi$. There is some analysis out there that can help. We will just state the results of this analysis without proof.\n",
    "\n",
    "Let $\\lambda_{\\mathrm{max}}(A^TA)$ be the largest eigenvalue of $A^TA$. The result is that as the number of samples $m \\rightarrow \\infty$, any choice of $\\phi$ that satisfies the following condition will _eventually_ converge to the best least-squares estimator of $x$, that is, the estimate of $x$ you would have gotten by solving the least squares with all the data.\n",
    "\n",
    "$$\n",
    "  0 < \\phi < \\frac{2}{\\lambda_{\\mathrm{max}}(A^TA)}.\n",
    "$$\n",
    "\n",
    "This condition is not very satisfying, because you cannot really know $\\lambda_{\\mathrm{max}}(A^TA)$ until you've seen all the data, whereas we would like to apply this procedure _online_ as the data arrive. Nevertheless, in practice you can imagine hybrid schemes that, given a batch of data points, use the QR fitting procedure to get a starting estimate for $x$ as well as to estimate a value of $\\phi$ to use for all future updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of the LMS algorithm\n",
    "\n",
    "To summarize, the algorithm is as follows:\n",
    "* Choose any initial guess, $x_0$, such as $x_0 \\leftarrow 0$.\n",
    "* For each observation $(b_k, a_k^T)$, do the update:\n",
    "\n",
    "  * $x_{k+1} \\leftarrow x_k + \\Delta_k$,\n",
    "  \n",
    "  where $\\Delta_k = \\phi \\cdot a_k (b_k - a_k^T x_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out the LMS idea\n",
    "\n",
    "Now _you_ should implement the LMS algorithm and see how it behaves.\n",
    "\n",
    "To start, let's generate an initial 1-D problem (2 regression coefficients, a slope and intercept), and solve it using the batch procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 100000\n",
    "d = 1\n",
    "x_true = generate_model (d)\n",
    "(A, b) = generate_data (m, x_true, sigma=0.1)\n",
    "\n",
    "print \"Condition number of the data matrix: \", np.linalg.cond (A)\n",
    "\n",
    "x = estimate_coeffs (A, b)\n",
    "e_rel = rel_diff (x, x_true)\n",
    "\n",
    "print \"Relative error:\", e_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we need a value for $\\phi$, for which we have an upper-bound of $\\lambda_{\\mathrm{max}}(A^TA)$. Let's cheat by computing it explicitly, even though in practice we would need to do something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LAMBDA_MAX = max (np.linalg.eigvals (A.T.dot (A)))\n",
    "print LAMBDA_MAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Implement the online LMS algorithm in the code cell below where indicated. It should produce a final parameter estimate, `x_lms`, as a column vector.\n",
    "\n",
    "In addition, the skeleton code below uses `rel_diff()` to record the relative difference between the estimate and the true vector, storing the $k$-th relative difference in `rel_diffs[k]`. Doing so will allow you to see the convergence behavior of the method.\n",
    "\n",
    "Lastly, to help you out, we've defined a constant in terms of $\\lambda_{\\mathrm{max}}(A^TA)$ that you can use for $\\phi$.\n",
    "\n",
    "> In practice, you would only maintain the current estimate, or maybe just a few recent estimates, rather than all of them. Since we want to inspect these vectors later, let's just store them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PHI = 1.99 / LAMBDA_MAX # Fudge factor\n",
    "rel_diffs = np.zeros ((m+1, 1))\n",
    "\n",
    "x_k = np.zeros ((d+1))\n",
    "for k in range (m):\n",
    "    rel_diffs[k] = rel_diff (x_k, x_true)\n",
    "    \n",
    "    # @YOUSE: Implement the online LMS algorithm.\n",
    "    # Use (b[k], A[k, :]) as the k-th observation.\n",
    "    pass\n",
    "    \n",
    "x_lms = x_k\n",
    "rel_diffs[m] = rel_diff (x_lms, x_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the true coefficients against the estimates, both from the batch algorithm and the online algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print x_true.T\n",
    "print x.T\n",
    "print x_lms.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the relative differences between each estimate `X[:, k]` and the true coefficients `x_true`, measured in the two-norm, to see if the estimate is converging to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot (range (len (rel_diffs)), rel_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if the dimension is `d=1`, let's go ahead and do a sanity-check regression fit plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STEP = A.shape[0] / 500\n",
    "if d == 1:\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot (A[::STEP, 1], b[::STEP], 'b+') # blue - data\n",
    "    ax1.plot (A[::STEP, 1], A.dot (x_true)[::STEP], 'r*') # red - true\n",
    "    ax1.plot (A[::STEP, 1], A.dot (x)[::STEP], 'go') # green - batch\n",
    "    ax1.plot (A[::STEP, 1], A.dot (x_lms)[::STEP], 'mo') # magenta - pure LMS\n",
    "else:\n",
    "    print \"Plot is multidimensional; I live in Flatland, so I don't do that.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Try playing with the dimensionality, number of points, the fudge factor $\\phi$, and the initial guess, to see how (or perhaps even whether) the online algorithm behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise.** We said previously that, in practice, you would probably do some sort of _hybrid_ scheme that mixes full batch updates (possibly only initially) and incremental updates. Try out such a scheme and describe what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
