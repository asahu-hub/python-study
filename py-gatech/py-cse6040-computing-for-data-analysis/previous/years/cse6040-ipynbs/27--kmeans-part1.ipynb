{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 6040, Fall 2015 [27]: K-means Clustering, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we studied the classification problem using the logistic regression algorithm. Since each data point needs to be labeled, it is called the **supervised learning** problem. However, in many applications, the data is not labeled. What can we do in this situation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning: find structures/patterns hidden in the unlabeled data\n",
    "**Clustering** is one of the most common tasks in unsupervised learning.\n",
    "\n",
    "Goal: Grouping similar objects together\n",
    "\n",
    "Input: Data $X = [x_1,\\dots, x_n]$,  number of clusters k\n",
    "\n",
    "Output: Partitioning $C_1,\\dots, C_k \\subset X$\n",
    "\n",
    "One needs to define an objective function to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering problem\n",
    "Given a set of data points $X = [x_1,\\dots, x_n]$, where each data point is a d-dimensional real vector, k-means clustering aims to partition the n observations into $k$ $(k\\le n)$ sets $C = \\{C_1,\\dots, C_k\\}$ so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center). In other words, its objective is to find:\n",
    "$$\n",
    "\\arg\\min_C \\sum_{i=1}^k \\sum_{x\\in C_i} ||x - \\mu_i||^2,\n",
    "$$\n",
    "where $\\mu_i$ is the mean of points in $C_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard K-means algorithm (Lloyd’s algorithm)\n",
    "Finding the solution is unfortunately NP-hard. Nevertheless, an iterative method known as Lloyd’s algorithm exists that converges (albeit to a local minimum) in few steps. The procedure alternates between two operations:\n",
    "\n",
    "**Assignment step:** Given a fixed set of k centers, assign each point to the nearest center in terms of Euclidean distance\n",
    "$$\n",
    "C_i = \\{x: ||x - \\mu_i || \\le ||x - \\mu_j ||, 1\\le j\\le k \\}\n",
    "$$\n",
    "\n",
    "**Update step:** Recompute k centroids by averaging all the data points belonging to each cluster (i.e. taking the means)\n",
    "$$\n",
    "\\mu_i = \\frac{1}{|C_i|} \\sum_{x\\in C_i} x\n",
    "$$\n",
    "\n",
    "<img src=\"https://dendroidblog.files.wordpress.com/2013/01/kmeansimg-scaled1000.jpg\" alt=\"K-means illustration\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same dataset from the last Lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv ('http://vuduc.org/cse6040/logreg_points_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x=\"x_1\", y=\"x_2\", hue=\"label\", fit_reg=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = df.as_matrix (['x_1', 'x_2'])\n",
    "labels = df['label'].as_matrix ()\n",
    "n = points.shape[0]\n",
    "d = points.shape[1]\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the labels should not be used in the k-means algorithm. It is only used here as the ground truth for later verification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize k centers\n",
    "The simplest way is to randomly select k data points from the data.\n",
    "Next time we will discuss more advanced initialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_centers(X, k):\n",
    "    # @YOUSE: randomly sample k data points as centers\n",
    "    # should return a (k x d) numpy array\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the distance matrix\n",
    "The most computational-costly step in k-means is computing the distances between each point to k centers. \n",
    "We can compute all the distances and store them in a (n x k) matrix. In order to calculate the objective function value later, we would like to store the square of Euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_d2(X, centers):\n",
    "    D = np.empty((n, k))\n",
    "    \n",
    "    # @YOUSE: fill D[i,j] as the square euclidean distance from point i to center j\n",
    "    pass\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Can you make it even faster? We will discuss about it next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering (grouping) based on the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_points(D):\n",
    "    # @YOUSE: return an (n x 1) numpy array which shows the assigned cluster for each point\n",
    "    # For example, D = [[0.3, 0.2],\n",
    "    #                   [0.1, 0.5],\n",
    "    #                   [0.4, 0.2]]\n",
    "    # should return np.array([1,0,1])    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the center of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_centers(X, clustering):\n",
    "    centers = np.empty((k, d))\n",
    "    for i in range(k):\n",
    "        # @YOUSE: compute the new center of cluster i  (centers[i, :])\n",
    "        pass\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WCSS(D):\n",
    "    # @YOUSE: return the objective function value (within-cluster sum of squares)\n",
    "    # For example, D = [[0.3, 0.2],\n",
    "    #                   [0.1, 0.5],\n",
    "    #                   [0.4, 0.2]]\n",
    "    # should return 0.2 + 0.1 + 0.2 = 0.5\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if k-means has already converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_converged(old_centers, centers):\n",
    "    # @YOUSE: return true if the k center points remain the same \n",
    "    # note that the ordering may be different\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans(X, k):\n",
    "    # @YOUSE: implement the k-means algorithm\n",
    "    # print the objective function value (WCSS) at each iteration until it converges\n",
    "    # return the final clustering\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clustering = kmeans(points, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the clustering result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['clustering'] = clustering\n",
    "sns.lmplot(data=df, x=\"x_1\", y=\"x_2\", hue=\"clustering\", fit_reg=False,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of misclustered points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "difference = np.sum(labels != clustering)\n",
    "error = min(difference, n - difference)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next time:\n",
    "1. More efficient implementation to compute the distance matrix.\n",
    "2. How to select k?\n",
    "3. Better way to initialize the centers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
