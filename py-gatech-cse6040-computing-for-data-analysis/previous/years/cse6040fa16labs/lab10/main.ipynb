{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "# Important note!\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "YOUR_ID = \"\" # Please enter your GT login, e.g., \"rvuduc3\" or \"gtg911x\"\n",
    "COLLABORATORS = [] # list of strings of your collaborators' IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b11295002cc2b9549d6a2b01721b6701",
     "grade": true,
     "grade_id": "who__test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_CHECK_ID = re.compile (r'''[a-zA-Z]+\\d+|[gG][tT][gG]\\d+[a-zA-Z]''')\n",
    "assert RE_CHECK_ID.match (YOUR_ID) is not None\n",
    "\n",
    "collab_check = [RE_CHECK_ID.match (i) is not None for i in COLLABORATORS]\n",
    "assert all (collab_check)\n",
    "\n",
    "del collab_check\n",
    "del RE_CHECK_ID\n",
    "del re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter / IPython version check.** The following code cell verifies that you are using the correct version of Jupyter/IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "Beyond regression, another important data analysis task is _classification_, in which you are given a set of labeled data points and you wish to learn a model of the labels. One technique you can apply is _logistic regression_, the topic of today's lab.\n",
    "\n",
    "> Although it's called \"regression\" it is really a model for classification.\n",
    "\n",
    "This notebook is about _binary classification_, where each data point belongs to one of $c=2$ possible classes. By convention, we will denote these _class labels_ by \"0\" and \"1.\" However, the ideas can be generalized to the multiclass ($c > 2$) case.\n",
    "\n",
    "Some of today's code snippets use Seaborn, so you may need to refer back the [optional notebook from `lab_bokeh`](http://nbviewer.ipython.org/github/rvuduc/cse6040fa16labs/blob/master/lab_bokeh/seaborn.ipynb) that explains how to use it.\n",
    "\n",
    "This lab builds on the iterative numerical optimization idea from [Lab 9](http://nbviewer.ipython.org/github/rvuduc/cse6040fa16labs/blob/master/lab9/part3.ipynb), which is known as _gradient ascent_ or _gradient descent_ (also, _steepest ascent/descent_), depending on whether one is maximizing or minimizing some quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note about slicing columns from a Numpy matrix.** If you want to extract a column `i` from a Numpy matrix `A` _and_ keep it as a column vector, you need to use the slicing notation, `A[:, i:i+1]`. Not doing so can lead to subtle bugs. To see why, compare the following slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array ([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9]\n",
    "              ], dtype=float)\n",
    "\n",
    "print (\"A[:, :] ==\\n\", A)\n",
    "print (\"\\nA[:, 0] ==\\n\", A[:, 0])\n",
    "print (\"\\nA[:, 2:3] == \\n\", A[:, 2:3])\n",
    "\n",
    "print (\"\\nAdd columns 0 and 2?\")\n",
    "a0 = A[:, 0]\n",
    "a1 = A[:, 2:3]\n",
    "print (a0 + a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example data: Rock lobsters!\n",
    "\n",
    "As a concrete example of a classification task, consider the results of [the following experiment](http://www.stat.ufl.edu/~winner/data/lobster_survive.txt).\n",
    "\n",
    "Some marine biologists took a bunch of lobsters of varying sizes (size being a proxy for stage of development), and then tethered and exposed these lobsters to a variety of predators. The outcome that they measured is whether the lobsters survived or not.\n",
    "\n",
    "The data is a set of points, one point per lobster, where there is a single predictor (the lobster's size) and the response is whether the lobsters survived (label \"1\") or died (label \"0\").\n",
    "\n",
    "> For the original paper, see [this link](http://downeastinstitute.org/assets/files/Published%20papers/Wilkinson%20et%20al%202015-1.pdf). For what we can only guess is what marine biologists look like in action, see [this (possibly NSFW) image](http://www.traemcneely.com/wp-content/uploads/2012/04/wpid-Lobster-Fights-e1335308484734.jpeg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data from: http://www.stat.ufl.edu/~winner/data/lobster_survive.dat\n",
    "df_lobsters = pd.read_table ('./lobster_survive.dat',\n",
    "                             sep=r'\\s+', names=['CarapaceLen', 'Survived'])\n",
    "display (df_lobsters.head ())\n",
    "print (\"...\")\n",
    "display (df_lobsters.tail ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot (x=\"Survived\", y=\"CarapaceLen\",\n",
    "                data=df_lobsters, inner=\"quart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the classes are distinct in the aggregate, where the median carapace (outer shell) length is around 36 mm for the lobsters that died and 42 mm for those that survived, they are not cleanly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "To develop some intuition and a classification algorithm, let's formulate the general problem and apply it to synthetic data sets.\n",
    "\n",
    "Let the data consist of $m$ observations of $d$ continuously-valued predictors. In addition, for each data observation we observe a binary label whose value is either 0 or 1.\n",
    "\n",
    "Just as in the linear regression case, represent each observation, or data point, by an _augumented_ vector, $\\hat{x}_i^T$,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\hat{x}_i^T\n",
    "    & \\equiv &\n",
    "      \\left(\\begin{array}{ccccc}\n",
    "           1 &\n",
    "        x_{i,1} &\n",
    "        x_{i,2} &\n",
    "         \\vdots &\n",
    "        x_{i,d}\n",
    "      \\end{array}\\right)\n",
    "      .\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, the point is the $d$ coordinates augmented by an initial dummy coordinate whose value is 1. This convention is similar to what we did in linear regression.\n",
    "\n",
    "We can also stack these points as rows of a matrix, $X$, again, just as we did in regression:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  X \\equiv\n",
    "    \\left(\\begin{array}{c}\n",
    "      \\hat{x}_0^T \\\\\n",
    "      \\hat{x}_1^T \\\\\n",
    "      \\vdots \\\\\n",
    "      \\hat{x}_{m-1}^T\n",
    "    \\end{array}\\right)\n",
    "  & = &\n",
    "    \\left(\\begin{array}{ccccc}\n",
    "      1 & x_{0,1} & x_{0,2} & \\cdots & x_{0,d} \\\\\n",
    "      1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,d} \\\\\n",
    "        &         &         & \\vdots & \\\\\n",
    "      1 & x_{m-1,1} & x_{m-1,2} & \\cdots & x_{m-1,d} \\\\\n",
    "    \\end{array}\\right).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We will take the labels to be a binary vector, $y^T \\equiv \\left(y_0, y_1, \\ldots, y_{m-1}\\right)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: A synthetic training set.** We've pre-generated a synethetic data set consisting of labeled data points. Let's download and inspect it, first as a table and then visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv ('./logreg_points_train.csv')\n",
    "\n",
    "display (df.head ())\n",
    "print (\"...\")\n",
    "display (df.tail ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_scatter_plot (df, x=\"x_1\", y=\"x_2\", hue=\"label\",\n",
    "                       palette={0: \"red\", 1: \"olive\"},\n",
    "                       size=5):\n",
    "    sns.lmplot (x=x, y=y, hue=hue, data=df, palette=palette,\n",
    "                fit_reg=False)\n",
    "    \n",
    "make_scatter_plot (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's extract the coordinates as a Numpy matrix of `points` and the labels as a Numpy column vector `labels`. Mathematically, the `points` matrix corresponds to $X$ and the `labels` vector corresponds to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = np.insert (df.as_matrix (['x_1', 'x_2']), 0, 1.0, axis=1)\n",
    "labels = df.as_matrix (['label'])\n",
    "\n",
    "print (\"First and last 5 points:\\n\", '='*23, '\\n', points[:5], '\\n...\\n', points[-5:], '\\n')\n",
    "print (\"First and last 5 labels:\\n\", '='*23, '\\n', labels[:5], '\\n...\\n', labels[-5:], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminants and the heaviside function\n",
    "\n",
    "Suppose you think that the _boundary_ between the two clusters may be represented by a line. For the synthetic data example above, I hope you'll agree that such a model is not a terrible one.\n",
    "\n",
    "A linear boundary is also known as a _linear discriminant_. Any point $x$ on this line may be described by $\\theta^T x$, where $\\theta$ is a vector of coefficients:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\theta\n",
    "    & \\equiv &\n",
    "      \\left(\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{array}\\right)\n",
    "      .\n",
    "      \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For example, suppose our observations have two predictors each ($d=2$). Let the corresponding data point be $x^T \\equiv (1.0, x_1, x_2)$. Then, $\\theta^T \\hat{x} = 0$ means that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "  &\n",
    "  \\theta^T x = 0\n",
    "  & = & \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 \\\\\n",
    "  \\implies\n",
    "  & x_2\n",
    "    & = & -\\frac{\\theta_0}{\\theta_2} - \\frac{\\theta_1}{\\theta_2} x_1.\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that describes points _on_ the line. However, given _any_ point $x$ in the $d$-dimensional space that is _not_ on the line, $\\theta^T x$ still produces a value: that value will be positive on one side of the line ($\\theta^T x > 0$) or negative on the other ($\\theta^T x < 0$).\n",
    "\n",
    "In other words, you can use the linear discriminant function, $\\theta^T x$, to _generate_ a label: just reinterpret its sign!\n",
    "\n",
    "One mathematical function that converts, say, a positive value to the label \"1\" and all other values to \"0\" is the _heaviside function_:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  H(y) & \\equiv & \\left\\{\\begin{array}{ll}\n",
    "      1 & \\mathrm{if}\\ y > 0\n",
    "      \\\\\n",
    "      0 & \\mathrm{if}\\ y \\leq 0\n",
    "    \\end{array}\\right..\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** (2 points). Given the a $m \\times (d+1)$ matrix of augmented points (i.e., the $X$ matrix) and a column vector $\\theta$ of length $d+1$, implement a function to compute the value of the linear discriminant at each point. That is, the function should return a (column) vector $y$ where the $y_i = \\theta^T x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "35997b77720fbbb9239ee53e5f549ec5",
     "grade": false,
     "grade_id": "lin_discr",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def lin_discr (X, theta):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b513b199a4d1d505defc32b8b7404d69",
     "grade": true,
     "grade_id": "lin_discr__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "theta_test = [random.random () for _ in range (3)]\n",
    "x1_test = [random.random () for _ in range (2)]\n",
    "x2_test = [(-theta_test[0] - theta_test[1]*x1) / theta_test[2] for x1 in x1_test]\n",
    "X_test = np.array ([[1.0, 1.0, 1.0, 1.0],\n",
    "                    [x1*2 for x1 in x1_test] + [x1*0.5 for x1 in x1_test],\n",
    "                    x2_test + x2_test]).T\n",
    "LD_test = lin_discr (X_test, np.array ([theta_test]).T)\n",
    "print (LD_test)\n",
    "assert (LD_test[:2] > 0).all ()\n",
    "assert (LD_test[2:] < 0).all ()\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** (2 points). Implement the _heaviside function_, $H(y)$. Your function should allow for an arbitrary _matrix_ of input values and should apply the heaviside function to each element.\n",
    "\n",
    "> There are several possible approaches that lead to one-line solutions. One uses only logical and arithmetic operators, which you will recall are implemented as elementwise operations for Numpy arrays. The other uses Numpy's [`sign()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.sign.html) function and transforms its output accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "fa1887aff2fc6ae99f8149f4d141a058",
     "grade": false,
     "grade_id": "heaviside",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def heaviside (Y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "508f4ff83592c458f6d1e5c75a3ef542",
     "grade": true,
     "grade_id": "heaviside__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Y_test = np.array ([[-2.3, 1.2, 7.],\n",
    "                    [0.0, -np.inf, np.inf]])\n",
    "H_Y_test = heaviside (Y_test)\n",
    "\n",
    "print (\"Y:\\n\", Y_test)\n",
    "print (\"\\nH(Y):\\n\", H_Y_test)\n",
    "\n",
    "assert (H_Y_test.astype (int) == np.array ([[0, 1, 1], [0, 0, 1]])).all ()\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next exercise, we'll need the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def heaviside_int (Y):\n",
    "    \"\"\"Evaluates the heaviside function, but returns integer values.\"\"\"\n",
    "    return heaviside (Y).astype (dtype=int)\n",
    "\n",
    "def gen_lin_discr_labels (points, theta, fun=heaviside_int):\n",
    "    \"\"\"\n",
    "    Given a set of points and the coefficients of a linear\n",
    "    discriminant, this function returns a set of labels for\n",
    "    the points with respect to this discriminant.\n",
    "    \"\"\"\n",
    "    score = lin_discr (points, theta)\n",
    "    labels = fun (score)\n",
    "    return labels\n",
    "\n",
    "def plot_lin_discr (theta, df, x=\"x_1\", y=\"x_2\", hue=\"label\",\n",
    "                    palette={0: \"red\", 1: \"olive\"}, size=5,\n",
    "                    linewidth=2):\n",
    "    lm = sns.lmplot (x=x, y=y, hue=hue, data=df, palette=palette,\n",
    "                     size=size, fit_reg=False)\n",
    "    \n",
    "    x_min, x_max = df[x].min (), df[x].max ()\n",
    "    y_min, y_max = df[y].min (), df[y].max ()\n",
    "    \n",
    "    x2_min = (-theta[0][0] - theta[1][0]*x_min) / theta[2][0]\n",
    "    x2_max = (-theta[0][0] - theta[1][0]*x_max) / theta[2][0]\n",
    "    plt.plot ([x_min, x_max], [x2_min, x2_max], linewidth=linewidth)\n",
    "    \n",
    "    def expand_interval (x_limits, percent=10.0):\n",
    "        x_min, x_max = x_limits[0], x_limits[1]\n",
    "        if x_min < 0:\n",
    "            x_min *= 1.0 + 1e-2*percent\n",
    "        else:\n",
    "            x_min *= 1.0 - 1e-2*percent\n",
    "        if x_max > 0:\n",
    "            x_max *= 1.0 + 1e-2*percent\n",
    "        else:\n",
    "            x_max *= 1.0 + 1e-2*percent\n",
    "        return (x_min, x_max)\n",
    "    x_view = expand_interval ((x_min, x_max))\n",
    "    y_view = expand_interval ((y_min, y_max))\n",
    "    lm.axes[0,0].set_xlim (x_view[0], x_view[1])\n",
    "    lm.axes[0,0].set_ylim (y_view[0], y_view[1])\n",
    "    \n",
    "def mark_matches (a, b, exact=False):\n",
    "    \"\"\"\n",
    "    Given two Numpy arrays of {0, 1} labels, returns a new boolean\n",
    "    array indicating at which locations the input arrays have the\n",
    "    same label (i.e., the corresponding entry is True).\n",
    "    \n",
    "    This function can consider \"inexact\" matches. That is, if `exact`\n",
    "    is False, then the function will assume the {0, 1} labels may be\n",
    "    regarded as the same up to a swapping of the labels. This feature\n",
    "    allows\n",
    "    \n",
    "      a == [0, 0, 1, 1, 0, 1, 1]\n",
    "      b == [1, 1, 0, 0, 1, 0, 0]\n",
    "      \n",
    "    to be regarded as equal. (That is, use `exact=False` when you\n",
    "    only care about \"relative\" labeling.)\n",
    "    \"\"\"\n",
    "    assert a.shape == b.shape\n",
    "    a_int = a.astype (dtype=int)\n",
    "    b_int = b.astype (dtype=int)\n",
    "    all_axes = tuple (range (len (a.shape)))\n",
    "    assert ((a_int == 0) | (a_int == 1)).all ()\n",
    "    assert ((b_int == 0) | (b_int == 1)).all ()\n",
    "    \n",
    "    exact_matches = (a_int == b_int)\n",
    "    if exact:\n",
    "        return exact_matches\n",
    "\n",
    "    assert exact == False\n",
    "    num_exact_matches = np.sum (exact_matches)\n",
    "    if (2*num_exact_matches) >= np.prod (a.shape):\n",
    "        return exact_matches\n",
    "    return exact_matches == False # Invert\n",
    "    \n",
    "def count_matches (a, b, exact=False):\n",
    "    \"\"\"\n",
    "    Given two sets of {0, 1} labels, returns the number of mismatches.\n",
    "    \n",
    "    This function can consider \"inexact\" matches. That is, if `exact`\n",
    "    is False, then the function will assume the {0, 1} labels may be\n",
    "    regarded as similar up to a swapping of the labels. This feature\n",
    "    allows\n",
    "    \n",
    "      a == [0, 0, 1, 1, 0, 1, 1]\n",
    "      b == [1, 1, 0, 0, 1, 0, 0]\n",
    "      \n",
    "    to be regarded as equal. (That is, use `exact=False` when you\n",
    "    only care about \"relative\" labeling.)\n",
    "    \"\"\"\n",
    "    matches = mark_matches (a, b, exact=exact)\n",
    "    return np.sum (matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** (2 points). For the synthetic data you loaded above, determine a value of $\\theta$ for which $H(\\theta^T x)$ \"best\" separates the two clusters. Store this $\\theta$ in a variable named `my_theta`, which should be a Numpy _column vector_. That is, define `my_theta` here using a line like:\n",
    "\n",
    "```python\n",
    "my_theta = np_col_vec ([0., -1., 3.])\n",
    "```\n",
    "\n",
    "where `np_col_vec` is defined below and the list of values are your best guesses at discriminating coefficients. The test code will check that your solution makes no more than ten misclassifications.\n",
    "\n",
    "> Hint: We found a set of coefficients that commits just 5 errors for the 375 input points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7759521a98a17cb81cf964ffded5c4c0",
     "grade": false,
     "grade_id": "my_theta",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def np_col_vec (list_values):\n",
    "    \"\"\"Returns a Numpy column vector for the given list of scalar values.\"\"\"\n",
    "    return np.array ([list_values]).T\n",
    "\n",
    "# Define `my_theta` as instructed above:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "461dc74e4f44123f1482759f54a3555e",
     "grade": true,
     "grade_id": "my_theta__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Here are the labels generated by your discriminant:\n",
    "my_labels = gen_lin_discr_labels (points, my_theta)\n",
    "\n",
    "# Here is a visual check:\n",
    "num_mismatches = len (labels) - count_matches (labels, my_labels)\n",
    "print (\"Detected\", num_mismatches, \"mismatches.\")\n",
    "\n",
    "df_matches = df.copy ()\n",
    "df_matches['label'] = mark_matches (my_labels, labels).astype (dtype=int)\n",
    "plot_lin_discr (my_theta, df_matches)\n",
    "\n",
    "assert num_mismatches <= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## An alternative discriminant: the logistic or \"sigmoid\" function\n",
    "\n",
    "The heaviside function, $H(\\theta^T x)$, enforces a sharp boundary between classes around the $\\theta^T x=0$ line. The following code produces a [contour plot](https://plot.ly/python/contour-plots/) to show this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = np.linspace (-2., +2., 100)\n",
    "x2 = np.linspace (-2., +2., 100)\n",
    "x1_grid, x2_grid = np.meshgrid (x1, x2)\n",
    "h_grid = heaviside (my_theta[0] + my_theta[1]*x1_grid + my_theta[2]*x2_grid)\n",
    "plt.contourf (x1, x2, h_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as the lobsters example suggests, real data are not likely to be cleanly separable, especially when the number of features we have at our disposal is relatively small.\n",
    "\n",
    "Since the labels are 0 or 1, you could look for a way to interpret labels as _probabilities_ rather than as hard (0 or 1) labels. The _logistic function_ provides a mathematical technique to do so:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  G(y) & \\equiv & \\dfrac{1}{1 + e^{-y}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> This function is also sometimes called the _logit_ or _sigmoid_ function.\n",
    "\n",
    "The logistic function takes any value in the range $(-\\infty, +\\infty)$ and produces a value in the range $(0, 1)$. Thus, given a value $x$, we can interpret it as a conditional probability that the label is 1 given $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** (2 points). Implement the logistic function. Inspect the resulting plot of $G(y)$ in 1-D and then the contour plot of $G(\\theta^T{x})$. Your function should accept a Numpy matrix of values, `Y`, and apply the sigmoid elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "843622dffe306476044b2e099f946635",
     "grade": false,
     "grade_id": "logistic",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic (Y):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# Plot your function for a 1-D input.\n",
    "y_values = np.linspace (-10, 10, 100)\n",
    "sns.regplot (y_values, logistic (y_values), fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b0eba830baa44f8c614564b449328553",
     "grade": true,
     "grade_id": "logistic__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert logistic (np.log (3)) == 0.75\n",
    "assert logistic (-np.log (3)) == 0.25\n",
    "\n",
    "g_grid = logistic (my_theta[0] + my_theta[1]*x1_grid + my_theta[2]*x2_grid)\n",
    "plt.contourf (x1, x2, g_grid)\n",
    "assert ((np.round (g_grid) - h_grid).astype (int) == 0).all ()\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** (do not need to turn in). Consider a set of 1-D points generated by a _mixture of Gaussians_. That is, suppose that there are two Gaussian distributions over the 1-dimensional variable, $x \\in (-\\infty, +\\infty)$, that have the _same_ variance ($\\sigma^2$) but _different_ means ($\\mu_0$ and $\\mu_1$). Show that the conditional probability of observing a point labeled \"1\" given $x$ may be written as,\n",
    "\n",
    "$$\\mathrm{Pr}\\left[l=1\\,|\\,x\\right]\n",
    "    \\propto \\dfrac{1}{1 + e^{-(\\theta_0 + \\theta_1 x)}},$$\n",
    "\n",
    "for a suitable definition of $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "_Hints._ Since the points come from Gaussian distributions,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}\\left[x \\, | \\, l\\right]\n",
    "    & \\equiv & \\dfrac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu_l)^2}{2 \\sigma^2}\\right).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To rewrite $\\mathrm{Pr}\\left[l\\,|\\,x\\right]$ in terms of $\\mathrm{Pr}\\left[x \\, | \\, l\\right]$, recall _Bayes's rule (also: Bayes's theorem)_:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[l=1\\,|\\,x]\n",
    "    & = &\n",
    "      \\dfrac{\\mathrm{Pr}[x\\,|\\,l=1] \\, \\mathrm{Pr}[l=1]}\n",
    "            {\\mathrm{Pr}[x]},\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where the denominator can be expanded as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[x] & = & \\mathrm{Pr}[x\\,|\\,l=0] \\, \\mathrm{Pr}[l=0] + \\mathrm{Pr}[x\\,|\\,l=1] \\, \\mathrm{Pr}[l=1].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "You may assume the prior probabilities of observing a 0 or 1 are given by $\\mathrm{Pr}[l=0] \\equiv p_0$ and $\\mathrm{Pr}[l=1] \\equiv p_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalizing to $d$-dimensions.** The preceding exercise can be generalized to $d$-dimensions. Let $\\theta$ and $x$ be $(d+1)$-dimensional points. Then,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}\\left[l=1\\,|\\,x\\right]\n",
    "    & \\propto & \\dfrac{1}{1 + \\exp \\left( -\\theta^T x \\right)}.\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5** (5 points). Verify the following properties of the logistic function, $G(y)$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcll}\n",
    "  G(y)\n",
    "    & = & \\frac{e^y}{e^y + 1}\n",
    "    & \\mathrm{(P1)} \\\\\n",
    "  G(-y)\n",
    "    & = & 1 - G(y)\n",
    "    & \\mathrm{(P2)} \\\\\n",
    "  \\dfrac{dG}{dy}\n",
    "    & = & G(y) G(-y)\n",
    "    & \\mathrm{(P3)} \\\\\n",
    "  {\\dfrac{d}{dy}} {\\left[ \\ln G(y) \\right]}\n",
    "    & = & G(-y)\n",
    "    & \\mathrm{(P4)} \\\\\n",
    "  {\\dfrac{d}{dy}} {\\ln \\left[ 1 - G(y) \\right]}\n",
    "    & = & -G(y)\n",
    "    & \\mathrm{(P5)}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b2eabdc601cd5eb7f83facacf144b01",
     "grade": true,
     "grade_id": "g_props",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the discriminant automatically via maximum likelihood estimation\n",
    "\n",
    "Previously, you determined $\\theta$ for our synthetic dataset experimentally. Can you compute a good $\\theta$ automatically? One of the standard techniques in statistics is to perform a _maximum likelihood estimation_ (MLE) of a model's parameters, $\\theta$. Indeed, you can use MLE to derive the normal equations for linear regression in a more \"statistically principled\" way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Likelihood\" as an objective function.** MLE derives from the following idea. Consider the joint probability of observing all of the labels, given the points and the parameters, $\\theta$:\n",
    "\n",
    "$$\n",
    "  \\mathrm{Pr}[y\\,|\\,X, \\theta].\n",
    "$$\n",
    "\n",
    "Suppose these observations are independent and identically distributed (i.i.d.). Then the joint probability can be factored as the product of individual probabilities,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[y\\, | \\,X, \\theta] = \\mathrm{Pr}[y_0, \\ldots, y_{m-1} \\,|\\, \\hat{x}_0, \\ldots, \\hat{x}_{m-1}, \\theta]\n",
    "  & = & \\mathrm{Pr}[y_0 \\,|\\, \\hat{x}_0, \\theta] \\cdots \\mathrm{Pr}[y_{m-1} \\,|\\, \\hat{x}_{m-1}, \\theta] \\\\\n",
    "  & = & \\displaystyle \\prod_{i=0}^{m-1} \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The _maximum likelihood principle_ says that you should try to choose $\\theta$ to maximize the chances (or \"likelihood\") of seeing these particular observations. Thus, you can simply reinterpret the preceding probability as an objective function to optimize (in this case, to maximize).\n",
    "\n",
    "For both mathematical and numerical reasons, we will use the _logarithm_ of the likelihood, or _log-likelihood_, as the objective function instead. Let's define it as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta; y, X)\n",
    "    & \\equiv &\n",
    "      \\log \\left\\{ \\displaystyle \\prod_{i=0}^{m-1} \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta] \\right\\} \\\\\n",
    "    & = &\n",
    "      \\displaystyle \\sum_{i=0}^{m-1} \\log \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta].\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "> We are using the symbol $\\log$, which could be taken in any convenient base, such as the natural logarithm ($\\ln y$) or the information theoretic base-two logarithm ($\\log_2 y$).\n",
    "\n",
    "The MLE fitting procedure then consists of two steps:\n",
    "\n",
    "* For the problem at hand, decide on a model of $\\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta]$.\n",
    "* Run any optimization procedure to find the $\\theta$ that maximizes $\\mathcal{L}(\\theta; y, X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Logistic regression.** Let's say you have decided that the logistic function, $G(\\hat{x}_i^T \\theta) = G(\\theta^T \\hat{x}_i)$, is a good model of the probability of producing a label $y_i$ given the observation $\\hat{x}_i^T$. Under the i.i.d. assumption, we can interpret the label $y_i$ as the result of flipping a coin, or a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial), where the probability of success ($y_i=1$) is defined as $g_i = g_i(\\theta) \\equiv G(\\hat{x}_i^T \\theta)$. Thus,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathrm{Pr}[y_i \\,|\\, \\hat{x}_i, \\theta]\n",
    "    & \\equiv & g_i^{y_i} \\cdot \\left(1 - g_i\\right)^{1 - y_i}.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The log-likelihood in turn becomes,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\mathcal{L}(\\theta; y, X)\n",
    "    & = & \\displaystyle\n",
    "      \\sum_{i=0}^{m-1} y_i \\ln g_i + (1-y_i) \\ln (1-g_i) \\\\\n",
    "    & = & \\displaystyle\n",
    "      \\sum_{i=0}^{m-1} y_i \\ln \\dfrac{g_i}{1-g_i} + \\ln (1-g_i) \\\\\n",
    "    & = & \\displaystyle\n",
    "      \\sum_{i=0}^{m-1} y_i \\theta^T \\hat{x}_i + \\ln (1-g_i).\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write the log-likelihood more compactly in a matrix-vector notation.\n",
    "\n",
    "**Convention 1.** Let $u \\equiv (1, \\ldots, 1)^T$ be a column vector of all ones, with its length inferred from context. Let $A = \\left(\\begin{array}{cccc} a_0 & a_1 & \\cdots & a_{n-1} \\end{array}\\right)$ be any matrix, where $\\{a_i\\}$ denote its $n$ columns. Then, the sum of the columns is\n",
    "\n",
    "$$\\sum_{i=0}^{n-1} a_i\n",
    "  = \\left(a_0\\ a_1\\ \\cdots\\ a_{n-1}\\right)\n",
    "      \\cdot \\left(\\begin{array}{c}\n",
    "              1 \\\\\n",
    "              1 \\\\\n",
    "              \\vdots \\\\\n",
    "              1\n",
    "            \\end{array}\\right)\n",
    "  = A u.\n",
    "$$\n",
    "\n",
    "**Convention 2.** Let $A = \\left(a_{ij}\\right)$ be any matrix and let $f(y)$ be any function that we have defined by default to accept a scalar argument $y$ and produce a scalar result. For instance, $f(y) = \\ln y$ or $f(y) = G(y)$. Then, assume that $B = f(A)$ applies $f(\\cdot)$ elementwise to $A$, returning a matrix $B$ whose elements $b_{ij} = f(a_{ij})$.\n",
    "\n",
    "With these notational conventions, convince yourself that these are two different ways to write the log-likelihood for logistic regression.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "  (\\mathrm{V1}) & \\mathcal{L}(\\theta; y, X) & = & y^T \\ln G(X \\theta) + (u-y)^T \\ln [u - G(X \\theta)] \\\\\n",
    "  (\\mathrm{V2}) & \\mathcal{L}(\\theta; y, X) & = & y^T X \\theta + u^T \\ln G(-X \\theta)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6** (2 points). Implement the log-likelihood function in Python by defining a function with the following signature:\n",
    "\n",
    "```python\n",
    "  def log_likelihood (theta, y, X):\n",
    "    ...\n",
    "```\n",
    "\n",
    "> To compute the elementwise logarithm of a matrix or vector, use Numpy's [`log`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9dfce5f1bab9b5cea3271ec28a6933e6",
     "grade": false,
     "grade_id": "log_likelihood",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "22ea8a721c45634f8d7d80b4c606135a",
     "grade": true,
     "grade_id": "log_likelihood__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "npzfile_soln = np.load ('log_likelihood_soln.npz')\n",
    "d_soln = npzfile_soln['arr_0']\n",
    "m_soln = npzfile_soln['arr_1']\n",
    "theta_soln = npzfile_soln['arr_2']\n",
    "y_soln = npzfile_soln['arr_3']\n",
    "X_soln = npzfile_soln['arr_4']\n",
    "L_soln = npzfile_soln['arr_5']\n",
    "\n",
    "L_you = log_likelihood (theta_soln, y_soln, X_soln)\n",
    "your_err = np.max (np.abs (L_you/L_soln - 1.0))\n",
    "display (Math (r'\\left\\|\\dfrac{\\mathcal{L}_{\\tiny \\mbox{yours}} - \\mathcal{L}_{\\tiny \\mbox{solution}}}{\\mathcal{L}_{\\tiny \\mbox{solution}}}\\right\\|_\\infty \\approx %g' % your_err))\n",
    "assert your_err <= 1e-13\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the MLE solution via gradient ascent: theory\n",
    "\n",
    "To optimize the log-likelihood with respect to the parameters, $\\theta$, you'd like to do the moral equivalent of taking its derivative, setting it to zero, and then solving for $\\theta$.\n",
    "\n",
    "For example, recall that in the case of linear regression via least squares minimization, carrying out this process produced an _analytic_ solution for the parameters, which was to solve the normal equations.\n",
    "\n",
    "Unfortunately, for logistic regression---or for most log-likelihoods you are likely to ever write down---you _cannot_ usually derive an analytic solution. Therefore, you will need to resort to numerical optimization procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient ascent, in 1-D.** The simplest procedure to maximize a function is _gradient ascent_ (or _steepest ascent_). If instead you are minimizing the function, then the equivalent procedure is gradient (or steepest) _descent_. Here is the basic idea in 1-D.\n",
    "\n",
    "Suppose we wish to find the maximum of a scalar function $f(x)$ in one dimension, i.e., $x$ is also a scalar. At the maximum, $\\dfrac{df(x)}{dx} = 0$.\n",
    "\n",
    "Suppose instead that $\\dfrac{df}{dx} \\neq 0$ and consider the value of $f$ at a nearby point, $x + p$, as given approximately by a truncated Taylor series:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  f(x + p)\n",
    "    & = &\n",
    "      f(x) + p \\dfrac{df(x)}{dx} + \\mathcal{O}(p^2).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To make progress toward maximizing $f(x)$, you'd like to choose $p$ so that $f(x+p) > f(x)$. One way is to choose $p=\\alpha \\cdot \\mathrm{sign} \\left(\\dfrac{df}{dx}\\right)$, where $0 < \\alpha \\ll 1$ is \"small:\"\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  f \\left(x + \\alpha \\cdot \\mathrm{sign} \\left(\\dfrac{df}{dx}\\right) \\right)\n",
    "    & \\approx &\n",
    "      f(x) + \\alpha \\left|\\dfrac{df}{dx}\\right| + \\mathcal{O}(\\alpha^2).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If $\\alpha$ is small enough, then you can neglect the $\\mathcal{O}(\\alpha^2)$ term and $f(x + p)$ will be larger than $f(x)$, thus making progress toward a maximum.\n",
    "\n",
    "This scheme is the basic idea: starting from some initial guess $x$, refine the guess by taking a small step $p$ _in the direction_ of the derivative, i.e., $\\mathrm{sign} \\left(\\dfrac{df}{dx}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient ascent in higher dimensions.** Now suppose $x$ is a vector rather than a scalar. Then the value of $f$ at a nearby point $f(x + p)$, where $p$ is a vector, becomes\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  f(x + p) = f(x) + p^T \\nabla_x f(x) + \\mathcal{O}(\\|p\\|^2),\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\nabla_x f(x)$ is the gradient of $f$ with respect to $x$. Just as in the 1-D case, you want a step $p$ such that $f(x + p) > f(x)$. To make as much progress as possible, let's choose $p$ to be parallel to $\\nabla_x\\,f(x)$, that is, proportional to the gradient. This intuition motivates the following choice of $p$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  p \\equiv \\alpha \\dfrac{\\nabla_x\\,f(x)}{\\|\\nabla_x\\,f(x)\\|}.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Again, $\\alpha$ is a kind of fudge factor. You need to choose it to be small enough that the high-order terms of the Taylor approximation become negligible, yet large enough that you can make reasonable progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient ascent applied to MLE.** Applying gradient ascent to the problem of maximizing the log-likelihood leads to the following algorithm.\n",
    "\n",
    "* Start with some initial guess, $\\theta(0)$.\n",
    "* At each iteration $t \\geq 0$ of the procedure, let $\\theta(t)$ be the current guess.\n",
    "* Compute the direction of steepest ascent by evaluating the gradient, $\\Delta_t \\equiv \\nabla_{\\theta(t)} \\left\\{\\mathcal{L}(\\theta(t); y, X)\\right\\}$.\n",
    "* Take a step in the direction of the gradient, $\\theta(t+1) \\leftarrow \\theta(t) + \\alpha \\dfrac{\\Delta_t}{\\|\\Delta_t\\|}$, where $\\alpha$ is a suitably chosen fudge factor.\n",
    "* Stop when the parameters don't change much or after some maximum number of steps.\n",
    "\n",
    "This procedure should smell eerily like the one in [Lab 9](http://nbviewer.ipython.org/github/rvuduc/cse6040fa16labs/blob/master/lab9/part3.ipynb)! And as was true of Lab 9, the tricky bit is how to choose $\\alpha$.\n",
    "\n",
    "> One additional and slight distinction between this procedure and the Lab 9 procedure is that here we are optimizing using the _full_ dataset rather than processing data points one at a time. (That is, the step iteration variable $t$ used above is not used in exactly the same way as the step iteration $k$ was used in Lab 9.)\n",
    ">\n",
    "> Another question is, how do we know this procedure will converge to the global maximum, rather than, say, a local maximum? For that you need a deeper analysis of a specific $\\mathcal{L}(\\theta; y, X)$, to show, for instance, that it is convex in $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing logistic regression using MLE by gradient ascent\n",
    "\n",
    "Let's apply the gradient ascent procedure to the logistic regression problem, in order to determine a good $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7** (3 points). Show the following.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\nabla_\\theta \\left\\{\\mathcal{L}(\\theta; y, X)\\right\\}\n",
    "    & = & X^T \\left[ y - G(X \\cdot \\theta)\\right].\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9a7ce35075292fd93f5b7779d3966b96",
     "grade": true,
     "grade_id": "grad_log_likelihood_math",
     "locked": false,
     "points": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8** (2 points). Implement a function to compute the gradient of the log-likelihood. Your function should have the signature,\n",
    "\n",
    "```python\n",
    "  def grad_log_likelihood (theta, y, X):\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "26b6c529701470ff29575d4916ffec0d",
     "grade": false,
     "grade_id": "grad_log_likelihood_code",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2d2a558dc353e1ee2bc6baebafcf1dd2",
     "grade": true,
     "grade_id": "grad_log_likelihood_code__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "npzfile_grad_soln = np.load ('grad_log_likelihood_soln.npz')\n",
    "d_grad_soln = npzfile_grad_soln['arr_0']\n",
    "m_grad_soln = npzfile_grad_soln['arr_1']\n",
    "theta_grad_soln = npzfile_grad_soln['arr_2']\n",
    "y_grad_soln = npzfile_grad_soln['arr_3']\n",
    "X_grad_soln = npzfile_grad_soln['arr_4']\n",
    "L_grad_soln = npzfile_grad_soln['arr_5']\n",
    "\n",
    "L_grad_you = grad_log_likelihood (theta_grad_soln, y_grad_soln, X_grad_soln)\n",
    "your_grad_err = np.max (np.abs (L_grad_you/L_grad_soln - 1.0))\n",
    "display (Math (r'\\left\\|\\dfrac{\\nabla\\, \\mathcal{L}_{\\tiny \\mbox{yours}} - \\nabla\\,\\mathcal{L}_{\\tiny \\mbox{solution}}}{\\nabla\\, \\mathcal{L}_{\\tiny \\mbox{solution}}}\\right\\|_\\infty \\approx %g' % your_grad_err))\n",
    "assert your_grad_err <= 1e-13\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9** (4 points). Implement the gradient ascent procedure to determine $\\theta$, and try it out on the sample data.\n",
    "\n",
    "In the code skeleton below, we've set up a loop to run a fixed number, `MAX_STEP`, of gradient ascent steps. Also, when normalizing the step $\\Delta_t$, use the two-norm.\n",
    "\n",
    "> In your solution, we'd like you to store *all* guesses in the matrix `thetas`, so that you can later see how the $\\theta(t)$ values evolve. To extract a particular column `t`, use the notation, `theta[:, t:t+1]`. This notation is necessary to preserve the \"shape\" of the column as a column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2e79d0dc0c79cfba791cefd76fbfac4f",
     "grade": false,
     "grade_id": "logreg_mle",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEP = 50\n",
    "ALPHA = 0.5\n",
    "\n",
    "# Get the data coordinate matrix, X, and labels vector, y\n",
    "X = points\n",
    "y = labels.astype (dtype=float)\n",
    "\n",
    "# Store *all* guesses, for subsequent analysis\n",
    "thetas = np.zeros ((3, MAX_STEP+1))\n",
    "\n",
    "for t in range (MAX_STEP):\n",
    "    # Fill in the code to compute thetas[:, t+1:t+2]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "theta_ga = thetas[:, MAX_STEP:]\n",
    "print (\"Your (hand) solution:\", my_theta.T.flatten ())\n",
    "print (\"Computed solution:\", theta_ga.T.flatten ())\n",
    "\n",
    "print (\"\\n=== Comparisons ===\")\n",
    "display (Math (r'\\dfrac{\\theta_0}{\\theta_2}:'))\n",
    "print (\"Your manual (hand-picked) solution is\", my_theta[0]/my_theta[2], \\\n",
    "      \", vs. MLE (via gradient ascent), which is\", theta_ga[0]/theta_ga[2])\n",
    "display (Math (r'\\dfrac{\\theta_1}{\\theta_2}:'))\n",
    "print (\"Your manual (hand-picked) solution is\", my_theta[1]/my_theta[2], \\\n",
    "      \", vs. MLE (via gradient ascent), which is\", theta_ga[1]/theta_ga[2])\n",
    "\n",
    "print (\"\\n=== The MLE solution, visualized ===\")\n",
    "ga_labels = gen_lin_discr_labels (points, theta_ga)\n",
    "df_ga = df.copy ()\n",
    "df_ga['label'] = mark_matches (ga_labels, labels).astype (dtype=int)\n",
    "plot_lin_discr (theta_ga, df_ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8b07e1c9ca90a6561a859c5711e2fde3",
     "grade": true,
     "grade_id": "logreg_mle__check",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print (\"\\n=== Mismatch counts ===\")\n",
    "\n",
    "my_labels = gen_lin_discr_labels (points, my_theta)\n",
    "my_mismatches = len (labels) - count_matches (labels, my_labels)\n",
    "print (\"Your manual (hand-picked) solution has\", num_mismatches, \"mismatches.\")\n",
    "\n",
    "ga_labels = gen_lin_discr_labels (points, theta_ga)\n",
    "ga_mismatches = len (labels) - count_matches (labels, ga_labels)\n",
    "print (\"The MLE method produces\", ga_mismatches, \"mismatches.\")\n",
    "\n",
    "assert ga_mismatches <= 8\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The gradient ascent trajectory.** Let's take a look at how gradient ascent progresses. (You might try changing the $\\alpha$ parameter and see how it affects the results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n1_ll = 100\n",
    "x1 = np.linspace (-20., 0., n1_ll)\n",
    "\n",
    "n2_ll = 100\n",
    "x2 = np.linspace (-20., 0., n2_ll)\n",
    "x1_grid, x2_grid = np.meshgrid (x1, x2)\n",
    "\n",
    "ll_grid = np.zeros ((n1_ll, n2_ll))\n",
    "for i1 in range (n1_ll):\n",
    "    for i2 in range (n2_ll):\n",
    "        theta_i1_i2 = np.array ([[thetas[0, MAX_STEP]],\n",
    "                                 [x1_grid[i1][i2]],\n",
    "                                 [x2_grid[i1][i2]]])\n",
    "        ll_grid[i1][i2] = log_likelihood (theta_i1_i2, y, X)\n",
    "\n",
    "p = plt.contourf (x1, x2, ll_grid, cmap=plt.cm.get_cmap(\"winter\"))\n",
    "plt.colorbar ()\n",
    "plt.plot (thetas[1, :], thetas[2, :], 'k*-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical optimization via Newton's method\n",
    "\n",
    "The fudge factor, $\\alpha$, in gradient ascent should give you pause. Can you choose the step size or direction in a better or more principled way?\n",
    "\n",
    "One idea is [_Newton's method_](http://www.math.uiuc.edu/documenta/vol-ismp/13_deuflhard-peter.pdf), summarized below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic idea, in 1-D.** Suppose you start at a point $x$ and, assuming you are not yet at the optimum, you wish to take a step $x + p$ so that $f(x + p)$ is the maximum.\n",
    "\n",
    "However, instead of trying to maximize $f(x + p)$ directly, let's replace $f(x + p)$ with some approximation $q(p)$, and then choose a $p$ to maximize $q(p)$. A simple choice for $q(p)$ is a _quadratic_ function in $p$. This choice is motivated by two factors: (a) since it's quadratic, it should have some sort of extreme point (and hopefully an actual maximum), and (b) it is a higher-order approximation than a linear one, and so hopefully more accurate than a linear one as well.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  f(x + p)\n",
    "    & \\approx & f(x) + p \\dfrac{df}{dx} + \\frac{1}{2} p^2 \\dfrac{d^2 f}{dx^2}\n",
    "    & \\equiv  & q(p).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To maximize $q(p)$, take its derivative and then solve for the $p_*$ such that $q(p_*) = 0$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\left.\\dfrac{dq}{dp}\\right|_{p=p_*}\n",
    "    & = & \\dfrac{df}{dx} + p_* \\dfrac{d^2 f}{dx^2} = 0 \\\\\n",
    "  \\implies p_*\n",
    "    & = & -\\dfrac{df}{dx} \\left(\\dfrac{d^2 f}{dx^2}\\right)^{-1}.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, the optimal step $p_*$ is the negative of the first derivative of $f$ divided by its second derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generalizing to higher dimensions.** To see how this procedure works in higher dimensions, you will need not only the gradient of $f(x)$, but also its _Hessian_, which is the moral equivalent of a second derivative.\n",
    "\n",
    "_Definition:_ **the Hessian.** Let $f(v)$ be a function that takes a _vector_ $v$ of length $n$ as input and returns a scalar. The _Hessian_ of $f(v)$ is an $n \\times n$ matrix, $H_v(f)$, whose entries are all $n^2$ possible second derivatives with respect to the components of $v$. That is, the $(i, j)$ element of $H_v(f)$ is given by $h_{ij}$ such that\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  h_{ij}\n",
    "    & \\equiv & \\dfrac{\\partial^2}{\\partial v_i \\partial v_j} f(v).\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Armed with a Hessian, the Newton step is defined as follows, by direct analogy to the 1-D case. First, the Taylor series approximation of $f(x + p)$ for multidimensional variables is, as it happens,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  f(x + p)\n",
    "    & \\approx & f(x) + {p^T \\, \\nabla_x \\, f} + {\\frac{1}{2}\\,p^T H_x(f) \\, p}\n",
    "    & \\equiv  & q(p).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "As in the 1-D case, we want to find an extreme point of $q(p)$. Taking its \"derivative\" (gradient), $\\nabla_p q$, and setting it to 0 yields,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\nabla_p \\, q(p)\n",
    "    & = & \\nabla_x \\, f(x) + H_x(f) \\, p = 0 \\\\\n",
    "  \\implies\n",
    "  H_x(f) \\cdot p\n",
    "    & = & -\\, \\nabla_x \\, f(x).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words, to choose the next step $p$, Newton's method suggests you must solve a system of linear equations, where the matrix is the Hessian of $f$ and the right-hand side is the negative gradient of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary: Newton's method.** Summarizing the main ideas from above, Newton's method to maximize the scalar objective function $f(x)$ where $x$ is a vector, consists of the following steps:\n",
    "\n",
    "* Start with some initial guess $x(0)$.\n",
    "* At step $t$, compute the _search direction_ $p(t)$ by solving $H_{x(t)}(f) \\cdot p(t) = -\\, \\nabla_x \\, f(x(t))$.\n",
    "* Compute a new (and hopefully improved) guess by the update, $x(t+1) \\leftarrow x(t) + p(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing logistic regression via a Newton-based MLE\n",
    "\n",
    "To perform MLE for the logistic regression model using Newton's method, you need both the gradient of the log-likelihood as well as the Hessian. You already know how to compute the gradient from the preceding exercises; so what about the Hessian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notationally, that calculation will be a little bit easier to write down and program with the following definition.\n",
    "\n",
    "_Definition:_ **Elementwise product**. Let $A \\equiv (a_{ij})$ and $B \\equiv (b_{ij})$ be $m \\times n$ matrices. Denote the _elementwise product_ of $A$ and $B$ by $A \\odot B$. That is, if $C = A \\odot B$, then element $c_{ij} = a_{ij} \\cdot b_{ij}$.\n",
    "\n",
    "If $A$ is $m \\times n$ but $B$ is instead just $m \\times 1$, then we will \"auto-extend\" $B$. Put differently, if $B$ has the same number of rows as $A$ but only 1 column, then we will take $C = A \\odot B$ to have elements $c_{ij} = a_{ij} \\cdot b_{i}$.\n",
    "\n",
    "In Python, you can use [`np.multiply()`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html) for elementwise multiplication of Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array ([[1, 2, 3],\n",
    "               [4, 5, 6]])\n",
    "B = np.array ([[-1, 2, -3],\n",
    "               [4, -5, 6]])\n",
    "\n",
    "print (np.multiply (A, B)) # elementwise product\n",
    "print ()\n",
    "print (np.multiply (A, B[:, 0:1])) # \"auto-extend\" version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Exercise** (do not need to turn anything in). Show that the Hessian of the log-likelihood for logistic regression is\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  H_{\\theta} \\left( \\mathcal{L}(\\theta; l, X) \\right)\n",
    "    & = & -\\left( X \\odot G(X \\theta) \\right)^T \\left( X \\odot G(-X \\theta) \\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10** (2 points). Implement a function to compute the Hessian of the log-likelihood. The signature of your function should be,\n",
    "\n",
    "```python\n",
    "  def hess_log_likelihood (theta, y, X):\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "338c0381652231df4674ff50f8f00bac",
     "grade": false,
     "grade_id": "hessian_log_likelihood",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "efde937aed4e80f22d2d9b33ac1ab925",
     "grade": true,
     "grade_id": "hess_log_likelihood__check",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "npzfile_hess_soln = np.load ('hess_log_likelihood_soln.npz')\n",
    "d_hess_soln = npzfile_hess_soln['arr_0']\n",
    "m_hess_soln = npzfile_hess_soln['arr_1']\n",
    "theta_hess_soln = npzfile_hess_soln['arr_2']\n",
    "y_hess_soln = npzfile_hess_soln['arr_3']\n",
    "X_hess_soln = npzfile_hess_soln['arr_4']\n",
    "L_hess_soln = npzfile_hess_soln['arr_5']\n",
    "\n",
    "L_hess_you = hess_log_likelihood (theta_hess_soln, y_hess_soln, X_hess_soln)\n",
    "your_hess_err = np.max (np.abs (L_hess_you/L_hess_soln - 1.0))\n",
    "display (Math (r'\\left\\|\\dfrac{H_{\\tiny \\mbox{yours}} - H_{\\tiny \\mbox{solution}}}{H_{\\tiny \\mbox{solution}}}\\right\\|_\\infty \\approx %g' % your_hess_err))\n",
    "assert your_hess_err <= 1e-13\n",
    "\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11** (4 points). Finish the implementation of a Newton-based MLE procedure for the logistic regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6646e27cdb4bedd2d4ce1fff47d03d43",
     "grade": false,
     "grade_id": "logreg_mle_newton",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "MAX_STEP = 10\n",
    "\n",
    "# Get the data coordinate matrix, X, and labels vector, l\n",
    "X = points\n",
    "y = labels.astype (dtype=float)\n",
    "\n",
    "# Store *all* guesses, for subsequent analysis\n",
    "thetas_newt = np.zeros ((3, MAX_STEP+1))\n",
    "\n",
    "for t in range (MAX_STEP):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "theta_newt = thetas_newt[:, MAX_STEP:]\n",
    "print (\"Your (hand) solution:\", my_theta.T.flatten ())\n",
    "print (\"Computed solution:\", theta_newt.T.flatten ())\n",
    "\n",
    "print (\"\\n=== Comparisons ===\")\n",
    "display (Math (r'\\dfrac{\\theta_0}{\\theta_2}:'))\n",
    "print (\"Your manual (hand-picked) solution is\", my_theta[0]/my_theta[2], \\\n",
    "      \", vs. MLE (via Newton's method), which is\", theta_newt[0]/theta_newt[2])\n",
    "display (Math (r'\\dfrac{\\theta_1}{\\theta_2}:'))\n",
    "print (\"Your manual (hand-picked) solution is\", my_theta[1]/my_theta[2], \\\n",
    "      \", vs. MLE (via Newton's method), which is\", theta_newt[1]/theta_newt[2])\n",
    "\n",
    "print (\"\\n=== The MLE solution, visualized ===\")\n",
    "newt_labels = gen_lin_discr_labels (points, theta_newt)\n",
    "df_newt = df.copy ()\n",
    "df_newt['label'] = mark_matches (newt_labels, labels).astype (dtype=int)\n",
    "plot_lin_discr (theta_newt, df_newt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "51a0fe7507afa8d274cc6480cc3de4d8",
     "grade": true,
     "grade_id": "logreg_mle_newt__check",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print (\"\\n=== Mismatch counts ===\")\n",
    "\n",
    "my_labels = gen_lin_discr_labels (points, my_theta)\n",
    "my_mismatches = len (labels) - count_matches (labels, my_labels)\n",
    "print (\"Your manual (hand-picked) solution has\", num_mismatches, \"mismatches.\")\n",
    "\n",
    "newt_labels = gen_lin_discr_labels (points, theta_newt)\n",
    "newt_mismatches = len (labels) - count_matches (labels, newt_labels)\n",
    "print (\"The MLE+Newton method produces\", newt_mismatches, \"mismatches.\")\n",
    "\n",
    "assert newt_mismatches <= ga_mismatches\n",
    "print (\"\\n(Passed.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "**Exercise 12** (1 point). The following cell creates a contour plot of the log-likelihood, as done previously in this notebook. Add code to display the trajectory taken by Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "021e9c20824fdb0e8a55cd9be6810b4c",
     "grade": true,
     "grade_id": "contour_logreg_mle_newton",
     "locked": false,
     "points": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "p = plt.contourf (x1, x2, ll_grid, cmap=plt.cm.get_cmap(\"winter\"))\n",
    "plt.colorbar ()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
