{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "solution": false
    }
   },
   "source": [
    "# Important note!\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "YOUR_ID = \"\" # Please enter your GT login, e.g., \"rvuduc3\" or \"gtg911x\"\n",
    "COLLABORATORS = [] # list of strings of your collaborators' IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b11295002cc2b9549d6a2b01721b6701",
     "grade": true,
     "grade_id": "who__test",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_CHECK_ID = re.compile (r'''[a-zA-Z]+\\d+|[gG][tT][gG]\\d+[a-zA-Z]''')\n",
    "assert RE_CHECK_ID.match (YOUR_ID) is not None\n",
    "\n",
    "collab_check = [RE_CHECK_ID.match (i) is not None for i in COLLABORATORS]\n",
    "assert all (collab_check)\n",
    "\n",
    "del collab_check\n",
    "del RE_CHECK_ID\n",
    "del re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter / IPython version check.** The following code cell verifies that you are using the correct version of Jupyter/IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: \"Online\" linear regression [12 points]\n",
    "\n",
    "The linear least squares algorithms we've discussed so far assume that you have all the samples you need before you go to estimate the model. But what if you only get to see samples \"one-at-a-time\" and you need to make predictions? We refer to such a model fitting procedure as being \"online\" or \"incremental\" (as opposed to \"offline\" or \"batched\").\n",
    "\n",
    "The neat thing about this procedure is that you can derive it using all the tools you already have at your disposal, namely, multivariate calculus.\n",
    "\n",
    "> This notebook uses yet another plotting facility known as [`matplotlib`](http://matplotlib.org/). Its interface is inspired by MATLAB but it is fairly low-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability with the problem size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, here is some code to help generate synthetic problems of a certain size, namely, $m \\times (n+1)$, where $m$ is the number of observations and $n$ the number of predictors. The $+1$ comes from our usual dummy coefficient for a non-zero intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_model (n):\n",
    "    \"\"\"Returns a set of (random) n+1 linear model coefficients.\"\"\"\n",
    "    return np.random.rand (n+1, 1)\n",
    "\n",
    "def generate_data (m, theta, sigma=1.0/(2**0.5)):\n",
    "    \"\"\"\n",
    "    Generates 'm' noisy observations for a linear model whose\n",
    "    predictor (non-intercept) coefficients are given in 'theta'.\n",
    "    Decrease 'sigma' to decrease the amount of noise.\n",
    "    \"\"\"\n",
    "    assert (type (theta) is np.ndarray) and (theta.ndim == 2) and (theta.shape[1] == 1)\n",
    "    n = len (theta)\n",
    "    X = np.random.rand (m, n)\n",
    "    X[:, 0] = 1.0\n",
    "    y = X.dot (theta) + sigma*np.random.randn (m, 1)\n",
    "    return (X, y)\n",
    "\n",
    "def estimate_coeffs (X, y):\n",
    "    \"\"\"\n",
    "    Solves X*theta = y by a linear least squares method.\n",
    "    \"\"\"\n",
    "    result = np.linalg.lstsq (X, y)\n",
    "    theta = result[0]\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_diff (x, y, ord=2):\n",
    "    \"\"\"\n",
    "    Computes ||x-y|| / ||y||. Uses 2-norm by default;\n",
    "    override by setting 'ord'.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm (x - y, ord=ord) / np.linalg.norm (y, ord=ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demo the above routines for a 2-D dataset.\n",
    "\n",
    "m = 50\n",
    "theta_true = generate_model (1)\n",
    "(X, y) = generate_data (m, theta_true, sigma=0.1)\n",
    "\n",
    "print (\"Dimensions of X:\", X.shape)\n",
    "print (\"Dimensions of theta_true:\", theta_true.shape)\n",
    "print (\"Dimensions of y:\", y.shape)\n",
    "\n",
    "print (\"Condition number of X: \", np.linalg.cond (X))\n",
    "print (\"True model coefficients:\", theta_true.T)\n",
    "\n",
    "theta = estimate_coeffs (X, y)\n",
    "\n",
    "print (\"Estimated model coefficients:\", theta.T)\n",
    "print (\"Relative error in the coefficients:\", rel_diff (theta, theta_true))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot (X[:, 1], y, 'b+') # Noisy observations\n",
    "ax1.plot (X[:, 1], X.dot (theta), 'r*') # Fit\n",
    "ax1.plot (X[:, 1], X.dot (theta_true), 'go') # True solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark varying $m$.** Let's benchmark the time to compute $x$ when the dimension $n$ of each point is fixed but the number $m$ of points varies. How does the running time scale with $m$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Benchmark, as 'm' varies:\n",
    "\n",
    "n = 32 # dimension\n",
    "M = [100, 1000, 10000, 100000, 1000000]\n",
    "times = [0.] * len (M)\n",
    "for (i, m) in enumerate (M):\n",
    "    theta_true = generate_model (n)\n",
    "    (X, y) = generate_data (m, theta_true, sigma=0.1)\n",
    "    t = %timeit -o estimate_coeffs (X, y)\n",
    "    times[i] = t.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_linear = [times[0]/M[0]*m for m in M]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.loglog (M, times, 'bo')\n",
    "ax1.loglog (M, t_linear, 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** (2 points). Now fix the number $m$ of observations but vary the dimension $n$. How does time scale with $n$? Complete the benchmark code below to find out. In particular, given the array `N[:]`, compute an array, `times[:]`, such that `times[i]` is the running time for a problem of size `m`$\\times$`(N[i]+1)`.\n",
    "\n",
    "> Hint: You can basically copy and modify the preceding benchmark. Also, note that the code cell following the one immediately below will plot your results against $\\mathcal{O}(n)$ and $\\mathcal{O}(n^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "38b1f920a24613b66649048feb19e32a",
     "grade": true,
     "grade_id": "bench_n",
     "locked": false,
     "points": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "N = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 3072]\n",
    "m = 5000\n",
    "times = [0.] * len (N)\n",
    "\n",
    "# Implement a benchmark to compute the time,\n",
    "# `times[i]`, to execute a problem of size `N[i]`.\n",
    "for (i, n) in enumerate (N):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_linear = [times[0]/N[0]*n for n in N]\n",
    "t_quadratic = [times[0]/N[0]/N[0]*n*n for n in N]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.loglog (N, times, 'bo')\n",
    "ax1.loglog (N, t_linear, 'r--')\n",
    "ax1.loglog (N, t_quadratic, 'g--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An online algorithm\n",
    "\n",
    "The empirical scaling appears to be pretty good, being roughly linear in $m$ or at worst quadratic in $n$. But there is still a downside in time and storage: each time there is a change in the data, you appear to need to form the data matrix all over again and recompute the solution from scratch, possibly touching the entire data set again!\n",
    "\n",
    "This approach, which requires the full data, is often referred to as a _batched_ or _offline_ procedure.\n",
    "\n",
    "This begs the question, is there a way to incrementally update the model coefficients whenever a new data point, or perhaps a small batch of new data points, arrives? Such a procedure would be considered _incremental_ or _online_, rather than batched or offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup: Key assumptions and main goal.** In the discussion that follows, assume that you only get to see the observations _one-at-a-time_. Let $(y_k, \\hat{x}_k^T)$ denote the current observation. (Relative to our previous notation, this tuple is just element $k$ of $y$ and row $k$ of $X$.\n",
    "\n",
    "> We will use $\\hat{x}_k^T$ to denote a row $k$ of $X$ since we previously used $x_j$ to denote column $j$ of $X$. That is,\n",
    ">\n",
    "> $$\n",
    "    X = \\left(\\begin{array}{ccc}\n",
    "          x_0 & \\cdots & x_{n}\n",
    "        \\end{array}\\right)\n",
    "      = \\left(\\begin{array}{c}\n",
    "          \\hat{x}_0^T \\\\\n",
    "            \\vdots \\\\\n",
    "          \\hat{x}_{m-1}^T\n",
    "        \\end{array}\\right),\n",
    "  $$\n",
    ">\n",
    "> where the first form is our previous \"columns-view\" representation and the second form is our \"rows-view.\"\n",
    "\n",
    "Additionally, assume that, at the time the $k$-th observation arrives, you start with a current estimate of the parameters, $\\hat{\\theta}(k)$, which is a vector. If for whatever reason you need to refer to element $i$ of that vector, use $\\hat{\\theta}_i(k)$. You will then compute a new estimate, $\\hat{\\theta}(k+1)$ using $\\hat{\\theta}(k)$ and $(y_k, \\hat{k}_k^T)$. For the discussion below, further assume that you throw out $\\hat{\\theta}(k)$ once you have $\\hat{\\theta}(k+1)$.\n",
    "\n",
    "As for your goal, recall that in the batch setting you start with _all_ the observations, $(y, X)$. From this starting point, you may estimate the linear regression model's parameters, $\\theta$, by solving $X \\theta = y$. In the online setting, you compute estimates one at a time. After seeing all $m$ observations in $X$, your goal is to compute an $\\hat{\\theta}_{m-1} \\approx \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An intuitive (but flawed) idea.** Indeed, there is a technique from the signal processing literature that we can apply to the linear regression problem, known as the least mean square (LMS) algorithm. Before describing it, let's start with an initial idea.\n",
    "\n",
    "Suppose that you have a current estimate of the parameters, $\\theta(k)$, when you get a new sample, $(y_k, \\hat{x}_k^T)$. The error in your prediction will be,\n",
    "\n",
    "$$y_k - \\hat{x}_k^T \\hat{\\theta}(k).$$\n",
    "\n",
    "Ideally, this error would be zero. So, let's ask if there exists a _correction_, $\\Delta_k$, such that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "     & y_k - \\hat{x}_k^T \\left( \\hat{\\theta}(k) + \\Delta_k \\right) & = & 0 \\\\\n",
    "\\iff &                           y_k - \\hat{x}_k^T \\hat{\\theta}(k) & = & \\hat{x}_k^T \\Delta_k\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Then, you could compute a new estimate of the parameter by $\\hat{\\theta}(k+1) = \\hat{\\theta}(k) + \\Delta_k$.\n",
    "\n",
    "This idea has a major flaw, which we will discuss below. But before we do, please try the following exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mental exercise 2 (no points -- you do not need to submit anything for this \"exercise.\").** Verify that the following choice of $\\Delta_k$ would make the preceding equation true.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "  \\Delta_k & = & \\dfrac{\\hat{x}_k}{\\|\\hat{x}_k\\|_2^2} \\left( y_k - \\hat{x}_k^T \\hat{\\theta}(k) \\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refining (or rather, \"hacking\") the basic idea: The least mean square (LMS) procedure.** The basic idea sketched above has at least one major flaw: the choice of $\\Delta_k$ might allow you to correctly predicts $y_k$ from $x_k$ and the new estimate $\\hat{\\theta}(k+1) = \\hat{\\theta}(k) + \\Delta_k$, but there is no guarantee that this new estimate $\\hat{\\theta}(k+1)$ preserves the quality of predictions made at all previous iterations!\n",
    "\n",
    "There are a number of ways to deal with this problem, which includes carrying out an update with respect to some (or all) previous data. However, there is also a simpler \"hack\" that, though it might require some parameter tuning, can be made to work in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That hack is as follows. Rather than using $\\Delta_k$ as computed above, let's compute a different update that has a \"fudge\" factor, $\\phi$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rrcl}\n",
    "  &\n",
    "  \\hat{\\theta}(k+1) & = & \\hat{\\theta}(k) + \\Delta_k\n",
    "  \\\\\n",
    "  \\mbox{where}\n",
    "  &\n",
    "  \\Delta_k & = & \\phi \\cdot \\hat{x}_k \\left( b_k - \\hat{x}_k^T \\hat{\\theta}(k) \\right).\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big question is how to choose $\\phi$. There is some analysis out there that can help. We will just state the results of this analysis without proof.\n",
    "\n",
    "Let $\\lambda_{\\mathrm{max}}(X^T X)$ be the largest eigenvalue of $X^T X$. The result is that as the number of samples $m \\rightarrow \\infty$, any choice of $\\phi$ that satisfies the following condition will _eventually_ converge to the best least-squares estimator of $\\hat{\\theta}$, that is, the estimate of $\\hat{\\theta}$ you would have gotten by solving the linear least squares problem with all of the data.\n",
    "\n",
    "$$\n",
    "  0 < \\phi < \\frac{2}{\\lambda_{\\mathrm{max}}(X^T X)}.\n",
    "$$\n",
    "\n",
    "This condition is not very satisfying, because you cannot really know $\\lambda_{\\mathrm{max}}(X^T X)$ until you've seen all the data, whereas we would like to apply this procedure _online_ as the data arrive. Nevertheless, in practice you can imagine hybrid schemes that, given a batch of data points, use the QR fitting procedure to get a starting estimate for $\\hat{\\theta}$ as well as to estimate a value of $\\phi$ to use for all future updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of the LMS algorithm.** To summarize, the algorithm is as follows:\n",
    "* Choose any initial guess, $\\hat{\\theta}(0)$, such as $\\hat{\\theta}(0) \\leftarrow 0$.\n",
    "* For each observation $(y_k, \\hat{x}_k^T)$, do the update:\n",
    "\n",
    "  * $\\hat{\\theta}(k+1) \\leftarrow \\hat{\\theta}_k + \\Delta_k$,\n",
    "  \n",
    "  where $\\Delta_k = \\phi \\cdot \\hat{x}_k \\left( y_k - \\hat{x}_k^T \\hat{\\theta}(k) \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out the LMS idea\n",
    "\n",
    "Now _you_ should implement the LMS algorithm and see how it behaves.\n",
    "\n",
    "To start, let's generate an initial 1-D problem (2 regression coefficients, a slope, and an intercept), and solve it using the batch procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = 100000\n",
    "n = 1\n",
    "theta_true = generate_model (n)\n",
    "(X, y) = generate_data (m, theta_true, sigma=0.1)\n",
    "\n",
    "print (\"Condition number of the data matrix:\", np.linalg.cond (X))\n",
    "\n",
    "theta = estimate_coeffs (X, y)\n",
    "e_rel = rel_diff (theta, theta_true)\n",
    "\n",
    "print (\"Relative error:\", e_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we need a value for $\\phi$, for which we have an upper-bound of $\\lambda_{\\mathrm{max}}(X^T X)$. Let's cheat by computing it explicitly, even though in practice we would need to do something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LAMBDA_MAX = max (np.linalg.eigvals (X.T.dot (X)))\n",
    "print (LAMBDA_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** (5 points). Implement the online LMS algorithm in the code cell below where indicated. It should produce a final parameter estimate, `theta_lms`, as a column vector.\n",
    "\n",
    "In addition, the skeleton code below uses `rel_diff()` to record the relative difference between the estimate and the true vector, storing the $k$-th relative difference in `rel_diffs[k]`. Doing so will allow you to see the convergence behavior of the method.\n",
    "\n",
    "Lastly, to help you out, we've defined a constant in terms of $\\lambda_{\\mathrm{max}}(X^T X)$ that you can use for $\\phi$.\n",
    "\n",
    "> In practice, you would only maintain the current estimate, or maybe just a few recent estimates, rather than all of them. Since we want to inspect these vectors later, go ahead and store them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5c1bfea584b660e98ca7820b4c26867e",
     "grade": true,
     "grade_id": "lms",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "PHI = 1.99 / LAMBDA_MAX # Fudge factor\n",
    "rel_diffs = np.zeros ((m+1, 1))\n",
    "\n",
    "theta_k = np.zeros ((n+1))\n",
    "for k in range (m):\n",
    "    rel_diffs[k] = rel_diff (theta_k, theta_true)\n",
    "\n",
    "    # Implement the online LMS algorithm.\n",
    "    # Use (y[k], X[k, :]) as the k-th observation.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "theta_lms = theta_k\n",
    "rel_diffs[m] = rel_diff (theta_lms, theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the true coefficients against the estimates, both from the batch algorithm and the online algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (theta_true.T)\n",
    "print (theta.T)\n",
    "print (theta_lms.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the relative differences between each estimate `Theta[:, k]` and the true coefficients `theta_true`, measured in the two-norm, to see if the estimate is converging to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot (range (len (rel_diffs)), rel_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if the dimension is `n=1`, let's go ahead and do a sanity-check regression fit plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STEP = X.shape[0] / 500\n",
    "if n == 1:\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.plot (X[::STEP, 1], y[::STEP], 'b+') # blue - data\n",
    "    ax1.plot (X[::STEP, 1], X.dot (theta_true)[::STEP], 'r*') # red - true\n",
    "    ax1.plot (X[::STEP, 1], X.dot (theta)[::STEP], 'go') # green - batch\n",
    "    ax1.plot (X[::STEP, 1], X.dot (theta_lms)[::STEP], 'mo') # magenta - pure LMS\n",
    "else:\n",
    "    print (\"Plot is multidimensional; I live in Flatland, so I don't do that.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise 4** (5 points). We said previously that, in practice, you would probably do some sort of _hybrid_ scheme that mixes full batch updates (possibly only initially) and incremental updates. Implement such a scheme and describe what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "11c1f27b9e7aaa0e602158e6c07dfc5e",
     "grade": true,
     "grade_id": "hybrid",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Setup problem and compute the batch solution\n",
    "m = 100000\n",
    "n = 1\n",
    "theta_true = generate_model (n)\n",
    "(X, y) = generate_data (m, theta_true, sigma=0.1)\n",
    "theta_batch = estimate_coeffs (X, y)\n",
    "\n",
    "# Your turn, below: Implement a hybrid batch-LMS solution\n",
    "# assuming you observe the first START data points all at\n",
    "# once initially, and then see the remaining points one\n",
    "# at a time.\n",
    "\n",
    "START = 100\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
